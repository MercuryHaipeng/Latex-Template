
@article{goldfarb_evaluating_2024,
	title = {Evaluating the {PyTorch} compiler in the vision domain},
	author = {Goldfarb, Aidan},
	date = {2024},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\JZH7G459\\Goldfarb - Evaluating the PyTorch Compiler in the Vision Domain.pdf:application/pdf},
}

@article{yuan_performance_2024,
	title = {Performance analysis of deep learning algorithms implemented using {PyTorch} in image recognition},
	volume = {247},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050924028084},
	doi = {10.1016/j.procs.2024.10.008},
	abstract = {With the rapid development of artificial intelligence technology, image recognition has become a key technology in many fields. This paperaims to analyze the performance of image recognition tasks through the implementation of deep learning algorithms using {PyTorch}. This paperintroduces the selection and preprocessing techniques of datasets, including normalization, data augmentation, size adjustment, and batch processing, which provide high-quality input data for model training. In terms of deep learning model design, Convolutional Neural Network was adopted, {CNN} was used as the infrastructure and the cross entropy loss function and Adam optimizer were selected for model training. The implementation details of {PyTorch} demonstrate the model construction, training process, and debugging, as well as how to use {PyTorch}'s {DataLoader} class to improve data loading efficiency. The Results and Discussion section presents the training results of the model, where the {CNN} model achieves the highest accuracy of 99\% in image recognition tasks, and the loss function value ultimately stabilizes at 0.02, indicating that the model has efficient learning and generalization capabilities. Performance comparison experiments show that, the {CNN} implemented by {PyTorch} outperforms {MXNet} and Caffe in terms of frame rate and processing delay, with a frame rate between 51FPS-79FPS and a minimum processing delay of 308ms. The analysis of practical application scenarios further explores the potential application of the model in fields such as medical image analysis, autonomous driving, retail industry, and safety monitoring, and points out challenges such as data privacy, model generalization ability, and computational resource limitations.},
	pages = {61--69},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Comput. Sci.},
	author = {Yuan, Jie},
	urldate = {2025-05-22},
	date = {2024},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\8TYXWA24\\Yuan - 2024 - Performance Analysis of Deep Learning Algorithms Implemented Using PyTorch in Image Recognition.pdf:application/pdf},
}

@inproceedings{riba_kornia_2020,
	location = {Snowmass Village, {CO}, {USA}},
	title = {Kornia: an open source differentiable computer vision library for {PyTorch}},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-7281-6553-0},
	url = {https://ieeexplore.ieee.org/document/9093363/},
	doi = {10.1109/WACV45572.2020.9093363},
	shorttitle = {Kornia},
	abstract = {This work presents Kornia – an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses {PyTorch} as its main backend both for efﬁciency and to take advantage of the reverse-mode auto-differentiation to deﬁne and compute the gradient of complex functions. Inspired by {OpenCV}, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as ﬁltering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.},
	eventtitle = {2020 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
	pages = {3663--3672},
	booktitle = {2020 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
	publisher = {{IEEE}},
	author = {Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary},
	urldate = {2025-05-22},
	date = {2020-03},
	langid = {english},
	note = {{TLDR}: Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations.},
	keywords = {Pytorch -{\textgreater} Komia},
	file = {Kornia - an open source differentiable computer vision library for pytorch:D\:\\Download\\Zotero\\storage\\26IZMTPU\\Kornia - an open source differentiable computer vision library for pytorch.pdf:application/pdf;PDF:D\:\\Download\\Zotero\\storage\\699RPLJQ\\Riba 等 - 2020 - Kornia an Open Source Differentiable Computer Vision Library for PyTorch.pdf:application/pdf},
}

@article{basha_efficient_2021,
	title = {An efficient face mask detector with {PyTorch} and deep learning},
	volume = {7},
	rights = {https://creativecommons.org/licenses/by/3.0/},
	issn = {2411-7145},
	url = {https://publications.eai.eu/index.php/phat/article/view/1222},
	doi = {10.4108/eai.8-1-2021.167843},
	abstract = {{INTRODUCTION}: The outbreak of a coronavirus disease in 2019 ({COVID}-19) has created a global health epidemic that has had a major effect on the way we view our environment and our daily lives. The Covid-19 affected numbers are rising at a tremendous pace. Because of that, many countries face an economic catastrophe, recession, and much more. One thing we should do is to separate ourselves from society, remain at home, and detach ourselves from the outside world. But that's no longer a choice, people need to earn to survive, and nobody can remain indefinitely within their homes. As a precaution, people should wear masks while keeping social distance, but some ignore such things and walk around.
{OBJECTIVES}: To develop a Face Mask Detector with {OpenCV}, {PyTorch}, and Deep Learning that helps to detect whether or not a person wears a mask.
{METHODS}: A Neural Network model called {ResNet} is trained on the dataset. Furthermore, this work makes use of the inbuilt Face Detector after training. Finally, we predict whether or not a person is wearing a mask along with the percentage of the face covered or uncovered.
{RESULTS}: The validation results have been proposed to be 97\% accurate when compared to applying different algorithms.
{CONCLUSION}: This Face Mask Detection System was found to be apt for detecting whether or not people wear masks in public places which contribute to their health and also to the health of their contacts in this {COVID}-19 pandemic.},
	pages = {e4},
	number = {25},
	journaltitle = {{EAI} Endorsed Transactions on Pervasive Health and Technology},
	shortjournal = {{EAI} Endorsed Trans. Pervasive Health Technol.},
	author = {Basha, Cmak. Zeelan and Pravallika, B.N. Lakshmi and Shankar, E. Bharani},
	urldate = {2025-05-22},
	date = {2021-01-08},
	langid = {english},
	note = {{TLDR}: This Face Mask Detection System was found to be apt for detecting whether or not people wear masks in public places which contribute to their health and also to the health of their contacts in this {COVID}-19 pandemic.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\I4KTQILX\\Basha 等 - 2021 - An Efficient Face Mask Detector with PyTorch and Deep Learning.pdf:application/pdf},
}

@article{dosovitskiy_image_2020,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2010.11929},
	doi = {10.48550/ARXIV.2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	journaltitle = {{arXiv} preprint {arXiv}:2010.11929},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2025-05-22},
	date = {2020},
	langid = {american},
	note = {publisher: {arXiv}
version: 2},
	keywords = {Transformer},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\ZUTYWRZD\\Dosovitskiy 等 - 2020 - An image is worth 16x16 words Transformers for image recognition at scale.pdf:application/pdf},
}

@article{carion_end--end_2020,
	title = {End-to-End Object Detection with Transformers},
	rights = {Creative Commons Zero v1.0 Universal},
	url = {https://arxiv.org/abs/2005.12872},
	doi = {10.48550/ARXIV.2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called {DEtection} {TRansformer} or {DETR}, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, {DETR} reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. {DETR} demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster {RCNN} baseline on the challenging {COCO} object detection dataset. Moreover, {DETR} can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	pages = {213--229},
	journaltitle = {European Conference on Computer Vision},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	urldate = {2025-05-22},
	date = {2020},
	langid = {american},
	doi = {10.1007/978-3-030-58452-8_13},
	note = {{ARXIV}\_ID: 2005.12872
{MAG} {ID}: 3096609285
S2ID: 962dc29fdc3fbdc5930a10aba114050b82fe5a3e
{TLDR}: This work presents a new method that views object detection as a direct set prediction problem, and demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster {RCNN} baseline on the challenging {COCO} object detection dataset.
publisher: {arXiv}
version: 3},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\BDGAJ627\\Nicolas Carion 等 - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@article{ashish_vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	pages = {5998--6008},
	journaltitle = {Neural Information Processing Systems},
	shortjournal = {Neural Inf. Process. Syst.},
	author = {{Ashish Vaswani} and Vaswani, Ashish and {Noam Shazeer} and Shazeer, Noam and {Niki Parmar} and Parmar, Niki and {Jakob Uszkoreit} and Uszkoreit, Jakob and {Llion Jones} and Jones, Llion and {Aidan N. Gomez} and Gomez, Aidan N. and {Łukasz Kaiser} and Kaiser, Lukasz and {Illia Polosukhin} and Polosukhin, Illia},
	urldate = {2025-05-22},
	date = {2017-06-12},
	langid = {american},
	note = {{ARXIV}\_ID: 1706.03762
{MAG} {ID}: 2963403868
S2ID: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
remark: 基于注意力机制的高效翻译模型
publisher: {arXiv}},
	keywords = {Transformer, ⭐⭐⭐⭐⭐, attention mechanism, machine translation, neural networks},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\YP5UMP75\\Ashish Vaswani 等 - 2017 - Attention is All you Need.pdf:application/pdf},
}

@online{noauthor_yolo_2025,
	title = {{YOLO} Model Comparison: {YOLOv}11 vs Previous},
	url = {https://www.ultralytics.com/blog/comparing-ultralytics-yolo11-vs-previous-yolo-models#},
	shorttitle = {{YOLO} Model Comparison},
	abstract = {Compare Ultralytics {YOLOv}8, {YOLOv}9, {YOLOv}10, and Ultralytics {YOLO}11 to understand how these models have evolved and improved from 2023 to 2025.},
	urldate = {2025-04-25},
	date = {2025},
	langid = {english},
	file = {Snapshot:D\:\\Download\\Zotero\\storage\\75YMZQMR\\comparing-ultralytics-yolo11-vs-previous-yolo-models.html:text/html},
}

@online{ultralytics_yolov8_2023,
	title = {{YOLOv}8 vs {YOLO}11: A Technical Comparison},
	url = {https://docs.ultralytics.com/compare/yolov8-vs-yolo11},
	shorttitle = {{YOLOv}8 vs {YOLO}11},
	abstract = {Compare {YOLOv}8 and {YOLO}11 for object detection. Explore their performance, architecture, and best-use cases to find the right model for your needs.},
	author = {Ultralytics},
	urldate = {2025-04-25},
	date = {2023},
	langid = {english},
	file = {Snapshot:D\:\\Download\\Zotero\\storage\\AKKFZKHQ\\yolov8-vs-yolo11.html:text/html},
}

@incollection{fleet_visualizing_2014,
	title = {Visualizing and Understanding Convolutional Networks},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the {ImageNet} benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the {ImageNet} classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our {ImageNet} model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	pages = {818--833},
	booktitle = {European Conference on Computer Vision},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	date = {2014-09-06},
	langid = {american},
	doi = {10.1007/978-3-319-10590-1_53},
	note = {{ARXIV}\_ID: 1311.2901
{MAG} {ID}: 1849277567
S2ID: 1a2a770d23b4a171fa81de62a78a3deb0588f238
{TLDR}: A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the {ImageNet} classification benchmark.
{bookTitle}: Computer Vision – {ECCV} 2014},
	keywords = {linter/error},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\7TJ85PTW\\Matthew D. Zeiler 等 - 2014 - Visualizing and Understanding Convolutional Networks.pdf:application/pdf},
}

@article{tian_y_l_key_2022,
	title = {Key issues in vision transformers: state of the art and prospects},
	volume = {48},
	issn = {0254-4156},
	url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2022&filename=moto202204002},
	shorttitle = {Key issues in vision transformers: state of the art and prospects},
	abstract = {Transformer所具备的长距离建模能力和并行计算能力使其在自然语言处理领域取得了巨大成功并逐步拓展至计算机视觉等领域.本文以分类任务为切入,介绍了典型视觉Transformer的基本原理和结构,并分析了Transformer与卷积神经网络在连接范围、权重动态性和位置表示能力三方面的区别与联系;同时围绕计算代价、性能提升、训练优化以及结构设计四个方面总结了视觉Transformer研究中的关键问题以及研究进展;并提出了视觉Transformer的一般性框架;然后针对检测和分割两个领域,介绍了视觉Transformer在特征学习、结果产生和真值分配等方面给上层视觉模型设计带来的启发和改变;并对视觉Transformer未来发展方向进行了展望.},
	pages = {957--979},
	number = {4},
	journaltitle = {Acta Automatica Sinica},
	shortjournal = {Acta Autom. Sin.},
	author = {{Tian Y. L} and {Wang Y. T} and {Wang J. G} and {Wang X} and {Wang F. Y}},
	urldate = {2025-04-27},
	date = {2022},
	langid = {english},
	note = {original-container-title: Acta Automatica Sinica
download: 3803
album: 电子技术及信息科学
{CLC}: {TP}391.41
{CNKICite}: 166
dbcode: {CJFD}
dbname: {CJFDLAST}2022
filename: moto202204002
{publicationTag}: 核心期刊, {JST}, {EI}, {CSCD}, {WJCI}, 卓越期刊
{CIF}: 6.013
{AIF}: 3.353},
	keywords = {计算机视觉, 目标检测, 视觉Transformer, 图像分割, 图像分类},
	file = {视觉Transformer研究的关键问题+现状及展望:D\:\\Download\\Zotero\\storage\\8WT264J2\\视觉Transformer研究的关键问题+现状及展望.pdf:application/pdf},
}

@article{zhao_review_2024,
	title = {A review of convolutional neural networks in computer vision},
	volume = {57},
	issn = {1573-7462},
	url = {https://link.springer.com/10.1007/s10462-024-10721-6},
	doi = {10.1007/s10462-024-10721-6},
	abstract = {Abstract
            In computer vision, a series of exemplary advances have been made in several areas involving image classification, semantic segmentation, object detection, and image super-resolution reconstruction with the rapid development of deep convolutional neural network ({CNN}). The {CNN} has superior features for autonomous learning and expression, and feature extraction from original input data can be realized by means of training {CNN} models that match practical applications. Due to the rapid progress in deep learning technology, the structure of {CNN} is becoming more and more complex and diverse. Consequently, it gradually replaces the traditional machine learning methods. This paper presents an elementary understanding of {CNN} components and their functions, including input layers, convolution layers, pooling layers, activation functions, batch normalization, dropout, fully connected layers, and output layers. On this basis, this paper gives a comprehensive overview of the past and current research status of the applications of {CNN} models in computer vision fields, e.g., image classification, object detection, and video prediction. In addition, we summarize the challenges and solutions of the deep {CNN}, and future research directions are also discussed.},
	pages = {99},
	number = {4},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Zhao, Xia and Wang, Limin and Zhang, Yufei and Han, Xuming and Deveci, Muhammet and Parmar, Milan},
	urldate = {2025-05-22},
	date = {2024-03-23},
	langid = {english},
	note = {Publisher: Springer
{TLDR}: An elementary understanding of {CNN} components and their functions, including input layers, convolution layers, pooling layers, activation functions, batch normalization, dropout, fully connected layers, and output layers are presented.},
	keywords = {{CNN}},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\7TQ5GU63\\Zhao 等 - 2024 - A review of convolutional neural networks in computer vision.pdf:application/pdf},
}

@article{khan_machine_2021,
	title = {Machine learning in computer vision: A review.},
	volume = {8},
	number = {32},
	journaltitle = {{EAI} Endorsed Transactions on Scalable Information Systems},
	shortjournal = {{EAI} Endorsed Trans. Scalable Inf. Syst.},
	author = {Khan, Abdullah Ayub and Laghari, Asif Ali and Awan, Shafique Ahmed},
	date = {2021},
	keywords = {⭐⭐⭐⭐⭐},
	file = {PDF:D\:\\Download\\Zotero\\storage\\QQNXMWGE\\Khan 等 - 2021 - Machine learning in computer vision A review..pdf:application/pdf},
}

@article{hussain_yolov5_2024,
	title = {Yolov5, yolov8 and yolov10: The go-to detectors for real-time vision},
	doi = {https://doi.org/10.48550/arXiv.2407.02988},
	journaltitle = {Arxiv Preprint Arxiv:2407.02988},
	shortjournal = {Arxiv Prepr. Arxiv:2407,02988},
	author = {Hussain, Muhammad},
	date = {2024},
	langid = {american},
	keywords = {linter/error},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\GDPC4IBS\\Hussain - 2024 - Yolov5, yolov8 and yolov10 The go-to detectors for real-time vision.pdf:application/pdf},
}

@article{schmidt_tracking_2024,
	title = {Tracking and mapping in medical computer vision: A review},
	volume = {94},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841524000562},
	doi = {10.1016/j.media.2024.103131},
	shorttitle = {Tracking and mapping in medical computer vision},
	pages = {103131},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Schmidt, Adam and Mohareri, Omid and {DiMaio}, Simon and Yip, Michael C. and Salcudean, Septimiu E.},
	urldate = {2025-05-22},
	date = {2024-05},
	langid = {english},
	note = {Publisher: {IEEE}
{TLDR}: This review provides an update to the field of camera-based tracking and scene mapping in surgery and diagnostics in medical computer vision and concludes that new methods need to be designed or combined to support clinical applications in deformable environments.},
	file = {已提交版本:D\:\\Download\\Zotero\\storage\\FAJ89G8V\\Awais 等 - 2025 - Foundation Models Defining a New Era in Vision a Survey and Outlook.pdf:application/pdf},
}

@misc{khanam_yolov11_2024,
	title = {{YOLOv}11: An Overview of the Key Architectural Enhancements},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2410.17725},
	doi = {10.48550/ARXIV.2410.17725},
	shorttitle = {{YOLOv}11},
	abstract = {This study presents an architectural analysis of {YOLOv}11, the latest iteration in the {YOLO} (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, {SPPF} (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores {YOLOv}11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection ({OBB}). We review the model's performance improvements in terms of mean Average Precision ({mAP}) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses {YOLOv}11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into {YOLOv}11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.},
	publisher = {{arXiv}},
	author = {Khanam, Rahima and Hussain, Muhammad},
	urldate = {2025-05-03},
	date = {2024},
	langid = {american},
	note = {Version Number: 1
{TLDR}: The paper explores {YOLOv}11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection ({OBB}), and reviews the model's performance improvements in terms of mean Average Precision ({mAP}) and computational efficiency compared to its predecessors.},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, linter/error},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\6FMVVLIP\\Khanam和Hussain - 2024 - YOLOv11 An Overview of the Key Architectural Enhancements.pdf:application/pdf},
}

@article{he_research_2025,
	title = {Research on object detection and recognition in remote sensing images based on {YOLOv}11},
	volume = {15},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-025-96314-x},
	doi = {10.1038/s41598-025-96314-x},
	abstract = {This study applies {theYOLOv}11 model to train and detect ground object targets in high-resolution remote sensing images, aiming to evaluate its potential in enhancing detection accuracy and efficiency. The model was trained on 70,389 samples across 20 target categories. After 496 training epochs, the loss functions (Box\_Loss, Cls\_Loss, and {DFL}\_Loss) demonstrated rapid convergence, indicating effective optimization in target localization, classification, and detail refinement. The evaluation metrics yielded a precision of 0.8861, a recall of 0.8563, a map50 of 0.8920, a map50–95 of 0.8646, and an F1 score of 0.8709, highlighting the model’s high accuracy and robustness in addressing complex detection tasks. Furthermore, 80\% of the test samples achieved confidence scores exceeding 85\%, confirming the reliability {ofYOLOv}11 in multiclass and multiobject detection scenarios. These findings suggest {thatYOLOv}11 holds significant promise for remote sensing image target detection, demonstrating exceptional detection performance while offering robust technical support for intelligent remote sensing image analysis. Future studies will focus on expanding the dataset, refining the model architecture, and improving its performance in detecting small targets and processing complex scenes, paving the way for its broader applications in environmental protection, urban planning, and multiobject detection.},
	pages = {14032},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {He, Lu-hao and Zhou, Yong-zhang and Liu, Lei and Cao, Wei and Ma, Jian-hua},
	urldate = {2025-05-22},
	date = {2025-04-23},
	langid = {english},
	note = {{TLDR}: {YOLOv}11 在遥感图像目标检测方面具有重要前景，在为智能遥感图像分析提供强大技术支持的同时，也展现了卓越的探测性能。},
	file = {PDF:D\:\\Download\\Zotero\\storage\\YFJD558E\\He 等 - 2025 - Research on object detection and recognition in remote sensing images based on YOLOv11.pdf:application/pdf},
}

@article{mao_yolo_2025,
	title = {{YOLO} Object Detection for Real-Time Fabric Defect Inspection in the Textile Industry: A Review of {YOLOv}1 to {YOLOv}11},
	volume = {25},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/7/2270},
	doi = {10.3390/s25072270},
	shorttitle = {{YOLO} Object Detection for Real-Time Fabric Defect Inspection in the Textile Industry},
	abstract = {Automated fabric defect detection is crucial for improving quality control, reducing manual labor, and optimizing efficiency in the textile industry. Traditional inspection methods rely heavily on human oversight, which makes them prone to subjectivity, inefficiency, and inconsistency in high-speed manufacturing environments. This review systematically examines the evolution of the You Only Look Once ({YOLO}) object detection framework from {YOLO}-v1 to {YOLO}-v11, emphasizing architectural advancements such as attention-based feature refinement and Transformer integration and their impact on fabric defect detection. Unlike prior studies focusing on specific {YOLO} variants, this work comprehensively compares the entire {YOLO} family, highlighting key innovations and their practical implications. We also discuss the challenges, including dataset limitations, domain generalization, and computational constraints, proposing future solutions such as synthetic data generation, federated learning, and edge {AI} deployment. By bridging the gap between academic advancements and industrial applications, this review is a practical guide for selecting and optimizing {YOLO} models for fabric inspection, paving the way for intelligent quality control systems.},
	pages = {2270},
	number = {7},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Mao, Makara and Hong, Min},
	urldate = {2025-05-22},
	date = {2025-04-03},
	langid = {english},
	note = {{TLDR}: 这篇综述系统地研究了 You Only Look Once （{YOLO}） 对象检测框架从 {YOLO}-v1 到 {YOLO}-v11 的演变，强调了基于注意力的特征优化和 Transformer 集成等架构进步及其对织物缺陷检测的影响。},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\RZSMETVF\\Mao和Hong - 2025 - YOLO Object Detection for Real-Time Fabric Defect Inspection in the Textile Industry A Review of YO.pdf:application/pdf},
}

@misc{hidayatullah_yolov8_2025,
	title = {{YOLOv}8 to {YOLO}11: A Comprehensive Architecture In-depth Comparative Review},
	rights = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2501.13400},
	doi = {10.48550/ARXIV.2501.13400},
	shorttitle = {{YOLOv}8 to {YOLO}11},
	abstract = {In the field of deep learning-based computer vision, {YOLO} is revolutionary. With respect to deep learning models, {YOLO} is also the one that is evolving the most rapidly. Unfortunately, not every {YOLO} model possesses scholarly publications. Moreover, there exists a {YOLO} model that lacks a publicly accessible official architectural diagram. Naturally, this engenders challenges, such as complicating the understanding of how the model operates in practice. Furthermore, the review articles that are presently available do not delve into the specifics of each model. The objective of this study is to present a comprehensive and in-depth architecture comparison of the four most recent {YOLO} models, specifically {YOLOv}8 through {YOLO}11, thereby enabling readers to quickly grasp not only how each model functions, but also the distinctions between them. To analyze each {YOLO} version's architecture, we meticulously examined the relevant academic papers, documentation, and scrutinized the source code. The analysis reveals that while each version of {YOLO} has improvements in architecture and feature extraction, certain blocks remain unchanged. The lack of scholarly publications and official diagrams presents challenges for understanding the model's functionality and future enhancement. Future developers are encouraged to provide these resources.},
	publisher = {{arXiv}},
	author = {Hidayatullah, Priyanto and Syakrani, Nurjannah and Sholahuddin, Muhammad Rizqi and Gelar, Trisna and Tubagus, Refdinal},
	urldate = {2025-05-03},
	date = {2025},
	langid = {american},
	note = {Version Number: 2
{TLDR}: 对四个最新的 {YOLO} 模型（特别是 {YOLOv}8 到 {YOLO}11）进行全面而深入的架构比较，从而使读者不仅可以快速掌握每个模型的功能，还可以快速掌握它们之间的区别。},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Important, linter/error},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\2YGRI52K\\Hidayatullah 等 - 2025 - YOLOv8 to YOLO11 A Comprehensive Architecture In-depth Comparative Review.pdf:application/pdf},
}

@misc{sapkota_yolov12_2024,
	title = {{YOLOv}12 to Its Genesis: A Decadal and Comprehensive Review of The You Only Look Once ({YOLO}) Series},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2406.19407},
	doi = {10.48550/ARXIV.2406.19407},
	shorttitle = {{YOLOv}12 to Its Genesis},
	abstract = {This review systematically examines the progression of the You Only Look Once ({YOLO}) object detection algorithms from {YOLOv}1 to the recently unveiled {YOLOv}12. Employing a reverse chronological analysis, this study examines the advancements introduced by {YOLO} algorithms, beginning with {YOLOv}12 and progressing through {YOLO}11 (or {YOLOv}11), {YOLOv}10, {YOLOv}9, {YOLOv}8, and subsequent versions to explore each version's contributions to enhancing speed, detection accuracy, and computational efficiency in real-time object detection. Additionally, this study reviews the alternative versions derived from {YOLO} architectural advancements of {YOLO}-{NAS}, {YOLO}-X, {YOLO}-R, {DAMO}-{YOLO}, and Gold-{YOLO}. By detailing the incremental technological advancements in subsequent {YOLO} versions, this review chronicles the evolution of {YOLO}, and discusses the challenges and limitations in each of the earlier versions. The evolution signifies a path towards integrating {YOLO} with multimodal, context-aware, and Artificial General Intelligence ({AGI}) systems for the next {YOLO} decade, promising significant implications for future developments in {AI}-driven applications. (Key terms: {YOLOv}12, {YOLOv}12 architecture, {YOLOv}11, {YOLO}11, {YOLO} Review, {YOLOv}14, {YOLOv}15, {YOLO} architecture, {YOLOv}12 architecture)},
	publisher = {{arXiv}},
	author = {Sapkota, Ranjan and Qureshi, Rizwan and Calero, Marco Flores and Badjugar, Chetan and Nepal, Upesh and Poulose, Alwin and Zeno, Peter and Vaddevolu, Uday Bhanu Prakash and Khan, Sheheryar and Shoman, Maged and Yan, Hong and Karkee, Manoj},
	urldate = {2025-05-04},
	date = {2024},
	langid = {american},
	note = {Version Number: 6
{TLDR}: The study highlights the transformative impact of {YOLO} across five critical application areas: automotive safety, healthcare, industrial manufacturing, surveillance, and agriculture.},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, {YOLOv}12, linter/error},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\GTKY3EQJ\\Sapkota 等 - 2024 - YOLOv12 to Its Genesis A Decadal and Comprehensive Review of The You Only Look Once (YOLO) Series.pdf:application/pdf},
}

@video{dr_priyanto_hidayatullah_yolo11_2024,
	title = {{YOLO}11 Architecture - Detailed Explanation},
	url = {https://www.youtube.com/watch?v=L9Va7Y9UT8E},
	author = {{Dr. Priyanto Hidayatullah}},
	urldate = {2025-05-04},
	date = {2024-10-28},
}

@article{dong_enhanced_2024,
	title = {An enhanced real-time human pose estimation method based on modified {YOLOv}8 framework},
	volume = {14},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-58146-z},
	doi = {10.1038/s41598-024-58146-z},
	abstract = {Abstract
            
              The objective of human pose estimation ({HPE}) derived from deep learning aims to accurately estimate and predict the human body posture in images or videos via the utilization of deep neural networks. However, the accuracy of real-time {HPE} tasks is still to be improved due to factors such as partial occlusion of body parts and limited receptive field of the model. To alleviate the accuracy loss caused by these issues, this paper proposes a real-time {HPE} model called
              
                
                  \$\$\{{\textbackslash}textbf \{{CCAM}-Person\}\}\$\$
                  
                    
                      {CCAM}
                      -
                      Person
                    
                  
                
              
              based on the {YOLOv}8 framework. Specifically, we have improved the backbone and neck of the {YOLOv}8x-pose real-time {HPE} model to alleviate the feature loss and receptive field constraints. Secondly, we introduce the context coordinate attention module ({CCAM}) to augment the model’s focus on salient features, reduce background noise interference, alleviate key point regression failure caused by limb occlusion, and improve the accuracy of pose estimation. Our approach attains competitive results on multiple metrics of two open-source datasets, {MS} {COCO} 2017 and {CrowdPose}. Compared with the baseline model {YOLOv}8x-pose, {CCAM}-Person improves the average precision by 2.8\% and 3.5\% on the two datasets, respectively.},
	pages = {8012},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Dong, Chengang and Du, Guodong},
	urldate = {2025-05-22},
	date = {2024-04-05},
	langid = {english},
	note = {{TLDR}: The backbone and neck of the {YOLOv}8x-pose real-time {HPE} model is improved and the context coordinate attention module ({CCAM}) is introduced to augment the model’s focus on salient features, reduce background noise interference, alleviate key point regression failure caused by limb occlusion, and improve the accuracy of pose estimation.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\PYF57HT7\\Dong和Du - 2024 - An enhanced real-time human pose estimation method based on modified YOLOv8 framework.pdf:application/pdf},
}

@article{hosna_transfer_2022,
	title = {Transfer learning: a friendly introduction},
	volume = {9},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w},
	doi = {10.1186/s40537-022-00652-w},
	shorttitle = {Transfer learning},
	abstract = {Infinite numbers of real-world applications use Machine Learning ({ML}) techniques to develop potentially the best data available for the users. Transfer learning ({TL}), one of the categories under {ML}, has received much attention from the research communities in the past few years. Traditional {ML} algorithms perform under the assumption that a model uses limited data distribution to train and test samples. These conventional methods predict target tasks undemanding and are applied to small data distribution. However, this issue conceivably is resolved using {TL}. {TL} is acknowledged for its connectivity among the additional testing and training samples resulting in faster output with efficient results. This paper contributes to the domain and scope of {TL}, citing situational use based on their periods and a few of its applications. The paper provides an in-depth focus on the techniques; Inductive {TL}, Transductive {TL}, Unsupervised {TL}, which consists of sample selection, and domain adaptation, followed by contributions and future directions.},
	pages = {102},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Hosna, Asmaul and Merry, Ethel and Gyalmo, Jigmey and Alom, Zulfikar and Aung, Zeyar and Azim, Mohammad Abdul},
	urldate = {2025-05-22},
	date = {2022-10-22},
	langid = {english},
	note = {{TLDR}: This paper provides an in-depth focus on the techniques; Inductive {TL}, Transductive {TL}, Unsupervised {TL}, which consists of sample selection, and domain adaptation, followed by contributions and future directions.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\TM4ABF8A\\Hosna 等 - 2022 - Transfer learning a friendly introduction.pdf:application/pdf},
}

@article{zhao_comparison_2024,
	title = {A comparison review of transfer learning and self-supervised learning: Definitions, applications, advantages and limitations},
	volume = {242},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423033092},
	doi = {10.1016/j.eswa.2023.122807},
	shorttitle = {A comparison review of transfer learning and self-supervised learning},
	abstract = {Deep learning has emerged as a powerful tool in various domains, revolutionising machine learning research. However, one persistent challenge is the scarcity of labelled training data, which hampers the performance and generalisation of deep learning models. To address this limitation, researchers have developed innovative methods to overcome data scarcity and enhance deep model learning capabilities. Two prevalent techniques that have gained significant attention are transfer learning and self-supervised learning. Transfer learning leverages knowledge learned from pre-training on a large-scale dataset, such as {ImageNet}, and applies it to a target task with limited labelled data. This approach allows models to benefit from the learned representations and effectively transfer knowledge to new tasks, resulting in improved learning performance and generalisation. On the other hand, self-supervised learning focuses on training models using pretext tasks that do not require manual annotation, allowing them to learn valuable representations from large amounts of unlabelled data. These learned representations can then be fine-tuned for downstream tasks, mitigating the need for extensive labelled data. In recent years, transfer and self-supervised learning have found applications in various fields, including medical image processing, video recognition, and natural language processing. These approaches have demonstrated remarkable achievements, enabling breakthroughs in areas such as disease diagnosis, object recognition, and language understanding. However, while these methods offer numerous advantages, they also have limitations. For example, transfer learning may face domain mismatch issues between the pre-training and target domains, while self-supervised learning requires careful design of pretext tasks to ensure meaningful representations. This review paper explores the recent applications of these pre-training methods in various fields within the past three years. It delves into the advantages and limitations of each approach, assesses the performance of models employing these techniques, and identifies potential directions for future research. By providing a comprehensive review of current pre-training methods, this article offers guidance for selecting the best technique for specific deep learning applications to address the data scarcity issue.},
	pages = {122807},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Zhao, Zehui and Alzubaidi, Laith and Zhang, Jinglan and Duan, Ye and Gu, Yuantong},
	urldate = {2025-05-22},
	date = {2024-05},
	langid = {english},
	keywords = {Transformer Learning, Self-supervisor Learning},
	file = {PDF:D\:\\Download\\Zotero\\storage\\AL9LFTIF\\Zhao 等 - 2024 - A comparison review of transfer learning and self-supervised learning Definitions, applications, ad.pdf:application/pdf},
}

@software{jocher_ultralytics_2024,
	title = {Ultralytics {YOLO}11},
	url = {https://github.com/ultralytics/ultralytics},
	version = {11.0.0},
	author = {Jocher, Glenn and Qiu, Jing},
	date = {2024},
}

@article{chen_fall_2020,
	title = {Fall Detection Based on Key Points of Human-Skeleton Using {OpenPose}},
	volume = {12},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/12/5/744},
	doi = {10.3390/sym12050744},
	abstract = {According to statistics, falls are the primary cause of injury or death for the elderly over 65 years old. About 30\% of the elderly over 65 years old fall every year. Along with the increase in the elderly fall accidents each year, it is urgent to find a fast and effective fall detection method to help the elderly fall.The reason for falling is that the center of gravity of the human body is not stable or symmetry breaking, and the body cannot keep balance. To solve the above problem, in this paper, we propose an approach for reorganization of accidental falls based on the symmetry principle. We extract the skeleton information of the human body by {OpenPose} and identify the fall through three critical parameters: speed of descent at the center of the hip joint, the human body centerline angle with the ground, and width-to-height ratio of the human body external rectangular. Unlike previous studies that have just investigated falling behavior, we consider the standing up of people after falls. This method has 97\% success rate to recognize the fall down behavior.},
	pages = {744},
	number = {5},
	journaltitle = {Symmetry},
	shortjournal = {Symmetry},
	author = {Chen, Weiming and Jiang, Zijie and Guo, Hailin and Ni, Xiaoyang},
	urldate = {2025-05-22},
	date = {2020-05-05},
	langid = {english},
	note = {{TLDR}: This paper proposes an approach for reorganization of accidental falls based on the symmetry principle that has 97\% success rate to recognize the fall down behavior and considers the standing up of people after falls.},
	file = {全文:D\:\\Download\\Zotero\\storage\\YVQG726H\\Chen 等 - 2020 - Fall Detection Based on Key Points of Human-Skeleton Using OpenPose.pdf:application/pdf},
}

@article{guo_survey_2025,
	title = {A Survey of the State of the Art in Monocular 3D Human Pose Estimation: Methods, Benchmarks, and Challenges},
	volume = {25},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/8/2409},
	doi = {10.3390/s25082409},
	shorttitle = {A Survey of the State of the Art in Monocular 3D Human Pose Estimation},
	abstract = {Three-dimensional human pose estimation (3D {HPE}) from monocular {RGB} cameras is a fundamental yet challenging task in computer vision, forming the basis of a wide range of applications such as action recognition, metaverse, self-driving, and healthcare. Recent advances in deep learning have significantly propelled the field, particularly with the incorporation of state-space models ({SSMs}) and diffusion models. However, systematic reviews that comprehensively cover these emerging techniques remain limited. This survey contributes to the literature by providing the first comprehensive analysis of recent innovative approaches, featuring diffusion models and {SSMs} within 3D {HPE}. It categorizes and analyzes various techniques, highlighting their strengths, limitations, and notable innovations. Additionally, it provides a detailed overview of commonly employed datasets and evaluation metrics. Furthermore, this survey offers an in-depth discussion on key challenges, particularly depth ambiguity and occlusion issues arising from single-view setups, thoroughly reviewing effective solutions proposed in recent studies. Finally, current applications and promising avenues for future research are highlighted to guide and inspire ongoing innovation in the area, with emerging trends such as integrating large language models ({LLMs}) to provide semantic priors and prompt-based supervision for improved 3D pose estimation.},
	pages = {2409},
	number = {8},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Guo, Yan and Gao, Tianhan and Dong, Aoshuang and Jiang, Xinbei and Zhu, Zichen and Wang, Fuxin},
	urldate = {2025-05-22},
	date = {2025-04-10},
	langid = {english},
	note = {{TLDR}: This survey contributes to the literature by providing the first comprehensive analysis of recent innovative approaches, featuring diffusion models and {SSMs} within 3D {HPE}, and provides a detailed overview of commonly employed datasets and evaluation metrics.},
	keywords = {Dataset},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\INYIEUA2\\Guo 等 - 2025 - A Survey of the State of the Art in Monocular 3D Human Pose Estimation Methods, Benchmarks, and Cha.pdf:application/pdf},
}

@article{sun_-depth_2025,
	title = {An In-Depth Analysis of 2D and 3D Pose Estimation Techniques in Deep Learning: Methodologies and Advances},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/14/7/1307},
	doi = {10.3390/electronics14071307},
	shorttitle = {An In-Depth Analysis of 2D and 3D Pose Estimation Techniques in Deep Learning},
	abstract = {Pose estimation ({PE}) is a cutting-edge technology in computer vision, essential for {AI}-driven sport analysis, advancing technological applications, enhancing security, and improving the quality of life. Deep learning has markedly advanced accuracy and efficiency in the field while propelling algorithmic frameworks and model architectures to greater complexity, yet rendering their underlying interrelations increasingly opaque. This review examines deep learning-based {PE} techniques, classifying them from two perspectives: two-dimensional (2D) and three-dimensional (3D), based on methodological principles and output formats. Within each category, advanced techniques for single-person, multi-person, and video-based {PE} are explored according to their applicable conditions, highlighting key differences and intrinsic connections while comparing performance metrics. We also analyze datasets across 2D, 3D, and video domains, with comparisons presented in tables. The practical applications of {PE} in daily life are also summarized alongside an exploration of the challenges facing the field and the proposal of innovative, forward-looking research directions. This review aims to be a valuable resource for researchers advancing deep learning-driven {PE}.},
	pages = {1307},
	number = {7},
	journaltitle = {Electronics},
	shortjournal = {Electronics},
	author = {Sun, Ruiyang and Lin, Zixiang and Leng, Song and Wang, Aili and Zhao, Lanfei},
	urldate = {2025-05-22},
	date = {2025-03-26},
	langid = {english},
	note = {{TLDR}: This review examines deep learning-based {PE} techniques from two perspectives: two-dimensional (2D) and three-dimensional (3D), based on methodological principles and output formats, and aims to be a valuable resource for researchers advancing deep learning-driven {PE}.},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\235ARFSW\\Sun 等 - 2025 - An In-Depth Analysis of 2D and 3D Pose Estimation Techniques in Deep Learning Methodologies and Adv.pdf:application/pdf},
}

@misc{medipelly_detection_2024,
	title = {Detection of Dark Web Threats Using Machine Learning and Image Processing},
	url = {http://arxiv.org/abs/2407.00704},
	doi = {10.48550/arXiv.2407.00704},
	abstract = {This paper aimed to discover the risks associated with the dark web and to detect the threats related to human trafficking using image processing with {OpenCV} and Python. Apart from that, a development environment was set up by installing {TensorFlow}, {OpenCV} and Python. Through exploratory data analysis ({EDA}), significant insights into the distribution and interactions of dataset features were obtained, which are crucial for evaluating various cyberthreats. The construction and evaluation of logistic regression and support vector machine ({SVM}) models revealed that the {SVM} model outperforms logistic regression in accuracy. The paper delves into the intricacies of data preprocessing, {EDA}, and model development, offering valuable insights into network protection and cyberthreat response.},
	number = {{arXiv}:2407.00704},
	publisher = {{arXiv}},
	author = {Medipelly, Swetha and Abosata, Nasr},
	urldate = {2025-05-11},
	date = {2024-06-30},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {2407.00704 [cs]},
	note = {{TLDR}: The construction and evaluation of logistic regression and support vector machine ({SVM}) models revealed that the {SVM} model outperforms logistic regression in accuracy, offering valuable insights into network protection and cyberthreat response.},
	keywords = {Computer Science - Cryptography and Security},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\APKEIUXS\\Medipelly和Abosata - 2024 - Detection of Dark Web Threats Using Machine Learning and Image Processing.pdf:application/pdf;Snapshot:D\:\\Download\\Zotero\\storage\\7NWWJBZ5\\2407.html:text/html},
}

@article{viswanathan_smart_2024,
	title = {Smart Attendance System using Face Recognition},
	volume = {11},
	rights = {https://creativecommons.org/licenses/by-nc-sa/4.0},
	issn = {2032-9407},
	url = {https://publications.eai.eu/index.php/sis/article/view/5203},
	doi = {10.4108/eetsis.5203},
	abstract = {Face recognition offers a wide range of valuable applications in social media, security, and surveillance contexts. The software used for building facial recognition algorithms is Python and {OpenCV}. "Attendance using Face Recognition" is a method for tracking and managing attendance that makes use of facial recognition technology. By seamlessly integrating the 'Face Recognition' module, a native Python feature, and the {OpenCV} library, our system excels in accuracy and dependability. The system then stores attendance records in a database and provides real-time reports. In this article, we demonstrate how to create a face recognition system in Python utilizing the built-in "Face Recognition" module and the {OpenCV} library. Our results show that our system achieves high accuracy and robustness while being efficient and scalable, catering to a wide spectrum of educational institutions, organizations, and enterprises.},
	number = {5},
	journaltitle = {{ICST} Transactions on Scalable Information Systems},
	shortjournal = {{ICST} Transactions on Scalable Information Systems},
	author = {Viswanathan, Jayaraj and E, Kuralamudhan and S, Navaneethan and S, Veluchamy},
	urldate = {2025-05-22},
	date = {2024-02-26},
	note = {{TLDR}: This article demonstrates how to create a face recognition system in Python utilizing the built-in "Face Recognition" module and the {OpenCV} library and shows that the system achieves high accuracy and robustness while being efficient and scalable, catering to a wide spectrum of educational institutions, organizations, and enterprises.},
	file = {全文:D\:\\Download\\Zotero\\storage\\D2PRIAQG\\Viswanathan 等 - 2024 - Smart Attendance System using Face Recognition.pdf:application/pdf},
}

@article{zhang_survey_2024,
	title = {A Survey on Visual Mamba},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/13/5683},
	doi = {10.3390/app14135683},
	abstract = {State space models ({SSM}) with selection mechanisms and hardware-aware architectures, namely Mamba, have recently shown significant potential in long-sequence modeling. Since the complexity of transformers’ self-attention mechanism is quadratic with image size, as well as increasing computational demands, researchers are currently exploring how to adapt Mamba for computer vision tasks. This paper is the first comprehensive survey that aims to provide an in-depth analysis of Mamba models within the domain of computer vision. It begins by exploring the foundational concepts contributing to Mamba’s success, including the {SSM} framework, selection mechanisms, and hardware-aware design. Then, we review these vision Mamba models by categorizing them into foundational models and those enhanced with techniques including convolution, recurrence, and attention to improve their sophistication. Furthermore, we investigate the widespread applications of Mamba in vision tasks, which include their use as a backbone in various levels of vision processing. This encompasses general visual tasks, medical visual tasks (e.g., 2D/3D segmentation, classification, image registration, etc.), and remote sensing visual tasks. In particular, we introduce general visual tasks from two levels: high/mid-level vision (e.g., object detection, segmentation, video classification, etc.) and low-level vision (e.g., image super-resolution, image restoration, visual generation, etc.). We hope this endeavor will spark additional interest within the community to address current challenges and further apply Mamba models in computer vision.},
	pages = {5683},
	number = {13},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Zhang, Hanwei and Zhu, Ying and Wang, Dan and Zhang, Lijun and Chen, Tianxiang and Wang, Ziyang and Ye, Zi},
	urldate = {2025-05-22},
	date = {2024-06-28},
	langid = {english},
	file = {全文:D\:\\Download\\Zotero\\storage\\HI2DA4Y3\\Zhang 等 - 2024 - A Survey on Visual Mamba.pdf:application/pdf},
}

@article{jia_human_2024,
	title = {Human image generation: a comprehensive survey},
	volume = {56},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3665869},
	doi = {10.1145/3665869},
	shorttitle = {Human image generation},
	abstract = {Image and video synthesis has become a blooming topic in computer vision and machine learning communities along with the developments of deep generative models, due to its great academic and application value. Many researchers have been devoted to synthesizing high-fidelity human images as one of the most commonly seen object categories in daily lives, where a large number of studies are performed based on various models, task settings, and applications. Thus, it is necessary to give a comprehensive overview on these variant methods on human image generation. In this article, we divide human image generation techniques into three paradigms, i.e., data-driven methods, knowledge-guided methods, and hybrid methods. For each paradigm, the most representative models and the corresponding variants are presented, where the advantages and characteristics of different methods are summarized in terms of model architectures. The main public human image datasets and evaluation metrics in the literature are also summarized. Furthermore, due to the wide application potential, the typical downstream usages of synthesized human images are covered. Finally, the challenges and potential opportunities of human image generation are discussed to shed light on future research.},
	pages = {1--39},
	number = {11},
	journaltitle = {{ACM} Comput. Surv.},
	shortjournal = {{ACM} Comput, Surv,},
	author = {Jia, Zhen and Zhang, Zhang and Wang, Liang and Tan, Tieniu},
	urldate = {2025-05-22},
	date = {2024-06},
	langid = {english},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery
{TLDR}: This paper divides human image generation techniques into three paradigms, i.e., data-driven methods, knowledge-guided methods and hybrid methods, where the advantages and characteristics of different methods are summarized in terms of model architectures.},
	keywords = {Dataset, 3D human body model, deep generative model, Human image generation, human image rendering, person image generation},
	file = {已提交版本:D\:\\Download\\Zotero\\storage\\JL8FDHTY\\Jia 等 - 2024 - Human Image Generation A Comprehensive Survey.pdf:application/pdf},
}

@article{zhang_segment_2024,
	title = {Segment anything model for medical image segmentation: Current applications and future directions},
	volume = {171},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482524003226},
	doi = {10.1016/j.compbiomed.2024.108238},
	shorttitle = {Segment anything model for medical image segmentation},
	pages = {108238},
	journaltitle = {Computers in Biology and Medicine},
	shortjournal = {Computers in Biology and Medicine},
	author = {Zhang, Yichi and Shen, Zhenrong and Jiao, Rushi},
	urldate = {2025-05-22},
	date = {2024-03},
	langid = {english},
	file = {已提交版本:D\:\\Download\\Zotero\\storage\\HWZC3XVC\\Zhang 等 - 2024 - Segment anything model for medical image segmentation Current applications and future directions.pdf:application/pdf;已提交版本:D\:\\Download\\Zotero\\storage\\F3XRFBXU\\Zhang 等 - 2024 - Segment anything model for medical image segmentation Current applications and future directions.pdf:application/pdf},
}

@misc{khanam_review_2025,
	title = {A Review of {YOLOv}12: Attention-Based Enhancements vs. Previous Versions},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2504.11995},
	doi = {10.48550/ARXIV.2504.11995},
	shorttitle = {A Review of {YOLOv}12},
	abstract = {The {YOLO} (You Only Look Once) series has been a leading framework in real-time object detection, consistently improving the balance between speed and accuracy. However, integrating attention mechanisms into {YOLO} has been challenging due to their high computational overhead. {YOLOv}12 introduces a novel approach that successfully incorporates attention-based enhancements while preserving real-time performance. This paper provides a comprehensive review of {YOLOv}12's architectural innovations, including Area Attention for computationally efficient self-attention, Residual Efficient Layer Aggregation Networks for improved feature aggregation, and {FlashAttention} for optimized memory access. Additionally, we benchmark {YOLOv}12 against prior {YOLO} versions and competing object detectors, analyzing its improvements in accuracy, inference speed, and computational efficiency. Through this analysis, we demonstrate how {YOLOv}12 advances real-time object detection by refining the latency-accuracy trade-off and optimizing computational resources.},
	publisher = {{arXiv}},
	author = {Khanam, Rahima and Hussain, Muhammad},
	urldate = {2025-05-14},
	date = {2025},
	langid = {american},
	note = {Version Number: 1
{TLDR}: This analysis demonstrates how {YOLOv}12 advances real-time object detection by refining the latency-accuracy trade-off and optimizing computational resources, and benchmarked {YOLOv}12 against prior {YOLO} versions and competing object detectors, analyzing its improvements in accuracy, inference speed, and computational efficiency.},
	keywords = {⭐⭐⭐⭐⭐, Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, {YOLOv}12, linter/error, attention mechanisms, real-time object detection},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\NGSWNMQX\\Khanam和Hussain - 2025 - A Review of YOLOv12 Attention-Based Enhancements vs. Previous Versions.pdf:application/pdf},
}

@misc{tian_yolov12_2025,
	title = {{YOLOv}12: Attention-Centric Real-Time Object Detectors},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2502.12524},
	doi = {10.48550/ARXIV.2502.12524},
	shorttitle = {{YOLOv}12},
	abstract = {Enhancing the network architecture of the {YOLO} framework has been crucial for a long time, but has focused on {CNN}-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of {CNN}-based models. This paper proposes an attention-centric {YOLO} framework, namely {YOLOv}12, that matches the speed of previous {CNN}-based ones while harnessing the performance benefits of attention mechanisms. {YOLOv}12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, {YOLOv}12-N achieves 40.6\% {mAP} with an inference latency of 1.64 ms on a T4 {GPU}, outperforming advanced {YOLOv}10-N / {YOLOv}11-N by 2.1\%/1.2\% {mAP} with a comparable speed. This advantage extends to other model scales. {YOLOv}12 also surpasses end-to-end real-time detectors that improve {DETR}, such as {RT}-{DETR} / {RT}-{DETRv}2: {YOLOv}12-S beats {RT}-{DETR}-R18 / {RT}-{DETRv}2-R18 while running 42\% faster, using only 36\% of the computation and 45\% of the parameters. More comparisons are shown in Figure 1.},
	publisher = {{arXiv}},
	author = {Tian, Yunjie and Ye, Qixiang and Doermann, David},
	urldate = {2025-05-14},
	date = {2025},
	langid = {american},
	note = {Version Number: 1
{TLDR}: This paper proposes an attention-centric {YOLO} framework, namely {YOLOv}12, that matches the speed of previous {CNN}-based ones while harnessing the performance benefits of attention mechanisms.},
	keywords = {⭐⭐⭐⭐⭐, Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), {YOLOv}12, linter/error, Attention Mechanisms, Real-time Object Detection},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\PZCVUUWR\\Tian 等 - 2025 - YOLOv12 Attention-Centric Real-Time Object Detectors.pdf:application/pdf},
}

@article{hwang_fd-yolo_2025,
	title = {{FD}-{YOLO}: A {YOLO} Network Optimized for Fall Detection},
	volume = {15},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/15/1/453},
	doi = {10.3390/app15010453},
	shorttitle = {{FD}-{YOLO}},
	abstract = {Falls are defined by the World Health Organization ({WHO}) as incidents in which an individual unintentionally falls to the ground or a lower level. Falls represent a serious public health issue, ranking as the second leading cause of death from unintentional injuries, following traffic accidents. While fall prevention is crucial, prompt intervention after a fall is equally necessary. Delayed responses can result in severe complications, reduced recovery potential, and a negative impact on quality of life. This study focuses on detecting fall situations using image-based methods. The fall images utilized in this research were created by combining three open-source datasets to enhance generalization and adaptability across diverse scenarios. Because falls must be detected promptly, the {YOLO} (You Only Look Once) network, known for its effectiveness in real-time detection, was applied. To better capture the complex body structures and interactions with the floor during a fall, two key techniques were integrated. First, a global attention module ({GAM}) based on the Convolutional Block Attention Module ({CBAM}) was employed to improve detection performance. Second, a Transformer-based Swin Transformer module was added to effectively learn global spatial information and enable a more detailed analysis of body movements. This study prioritized minimizing missed fall detections (false negatives, {FN}) as the key performance metric, since undetected falls pose greater risks than false detections. The proposed Fall Detection {YOLO} ({FD}-{YOLO}) network, developed by integrating the Swin Transformer and {GAM} into {YOLOv}9, achieved a high {mAP}@0.5 score of 0.982 and recorded only 134 missed fall incidents, demonstrating optimal performance. When implemented in environments equipped with standard camera systems, the proposed {FD}-{YOLO} network is expected to enable real-time fall detection and prompt post-fall responses. This technology has the potential to significantly improve public health and safety by preventing fall-related injuries and facilitating rapid interventions.},
	pages = {453},
	number = {1},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Hwang, Hoseong and Kim, Donghyun and Kim, Hochul},
	urldate = {2025-05-22},
	date = {2025-01-06},
	langid = {english},
	note = {{TLDR}: This study prioritized minimizing missed fall detections (false negatives, {FN}) as the key performance metric, since undetected falls pose greater risks than false detections.},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\L5MKRCLY\\Hwang 等 - 2025 - FD-YOLO A YOLO Network Optimized for Fall Detection.pdf:application/pdf},
}

@article{gan_generative_2025,
	title = {Generative Adversarial Networks with Learnable Auxiliary Module for Image Synthesis},
	volume = {21},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3653021},
	doi = {10.1145/3653021},
	abstract = {Training generative adversarial networks ({GANs}) for noise-to-image synthesis is a challenge task, primarily due to the instability of {GANs}’ training process. One of the key issues is the generator’s sensitivity to input data, which can cause sudden fluctuations in the generator’s loss value with certain inputs. This sensitivity suggests an inadequate ability to resist disturbances in the generator, causing the discriminator’s loss value to oscillate and negatively impacting the discriminator. Then, the negative feedback of discriminator is also not conducive to updating generator’s parameters, leading to suboptimal image generation quality. In response to this challenge, we present an innovative {GANs} model equipped with a learnable auxiliary module that processes auxiliary noise. The core objective of this module is to enhance the stability of both the generator and discriminator throughout the training process. To achieve this target, we incorporate a learnable auxiliary penalty and an augmented discriminator, designed to control the generator and reinforce the discriminator’s stability, respectively. We further apply our method to the Hinge and {LSGANs} loss functions, illustrating its efficacy in reducing the instability of both the generator and the discriminator. The tests we conducted on {LSUN}, {CelebA}, Market-1501, and Creative Senz3D datasets serve as proof of our method’s ability to improve the training stability and overall performance of the baseline methods.},
	pages = {1--21},
	number = {4},
	journaltitle = {{ACM} Transactions on Multimedia Computing, Communications, and Applications},
	shortjournal = {{ACM} Trans. Multimedia Comput. Commun. Appl.},
	author = {Gan, Yan and Yang, Chenxue and Ye, Mao and Huang, Renjie and Ouyang, Deqiang},
	urldate = {2025-05-22},
	date = {2025-04-30},
	langid = {english},
	note = {{TLDR}: This work presents an innovative {GANs} model equipped with a learnable auxiliary module that processes auxiliary noise, designed to enhance the stability of both the generator and discriminator throughout the training process.},
}

@online{ultralytics_yolo12__2025,
	title = {{YOLO}12\_ Attention-Centric Object Detection - Ultralytics {YOLO} Docs},
	url = {https://docs.ultralytics.com/zh/models/yolo12/#detection-performance-coco-val2017},
	author = {Ultralytics},
	urldate = {2025-05-16},
	date = {2025},
}

@article{noauthor_classroom_2025,
	title = {Classroom Attention Detection Based on Computer Vision and Artificial Intelligence},
	volume = {6},
	issn = {27900932},
	url = {http://scholar-press.com/papers/1983},
	doi = {10.38007/IJBDIT.2025.060103},
	number = {1},
	journaltitle = {International Journal of Big Data Intelligent Technology},
	shortjournal = {{IJBDIT}},
	urldate = {2025-05-22},
	date = {2025-02-28},
	langid = {american},
	note = {{TLDR}: The attention detection model proposed in this paper included three modules: gaze point estimation, gaze target recognition and attention level analysis, which used a deep learning algorithm that combines saliency detection with attention shifting.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\TYW7UIRB\\2025 - Classroom Attention Detection Based on Computer Vision and Artificial Intelligence.pdf:application/pdf},
}

@article{rani_mobilenet_2025,
	title = {{MobileNet} for human activity recognition in smart surveillance using transfer learning},
	volume = {37},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-024-10882-z},
	doi = {10.1007/s00521-024-10882-z},
	abstract = {Human activity recognition is a signiﬁcant and trending research area in computer vision due to its wide range of applications in healthcare, wellness, smart home systems, smart surveillance, and more. Recently, with the rise and successful deployment of deep learning techniques for image classiﬁcation and object recognition, research has shifted from traditional handcrafted methods to deep learning approaches. Video content has gained considerable popularity on the internet, especially on social media platforms like {YouTube}, spurring growing interest in video understanding within the research community. This paper analyzes the performance of the {MobileNet} model for human activity recognition in smart surveillance using transfer learning, a promising approach for leveraging pretrained models. The {MobileNet} model, pretrained on {ImageNet} with 101 classes, was modiﬁed with additional layers to enhance its performance. The model was further tested using the {UCF}101 benchmark dataset, which includes {RGB} images across 101 categories of activities, such as playing guitar, jumping, and typing. The primary goal is to assess the {MobileNet} model’s performance in improving human action recognition through transfer learning. Key metrics, including accuracy, precision, F1-score, Area under the Curve ({AUC}), and Root Mean Squared Error, are used to evaluate its effectiveness. The {MobileNet} model achieved an accuracy of 98.47\% on the {UCF}101 dataset. The experimental results demonstrate that the proposed technique is both robust and efﬁcient on the {UCF}101 dataset.},
	pages = {3907--3924},
	number = {5},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput. Appl.},
	author = {Rani, Manjot and Kumar, Munish},
	urldate = {2025-05-22},
	date = {2025-02},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\A2QI633U\\Rani和Kumar - 2025 - MobileNet for human activity recognition in smart surveillance using transfer learning.pdf:application/pdf},
}

@article{wen_mf-yolo_2025,
	title = {{MF}-{YOLO}: mask wearing detection algorithm for dense environments},
	volume = {13},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10929041/},
	doi = {10.1109/ACCESS.2025.3551892},
	shorttitle = {Mf-yolo},
	pages = {54601--54610},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Wen, Peng and Yuan, Zhengyi and Zhang, Junhu and Li, Haitao},
	urldate = {2025-05-22},
	date = {2025},
	langid = {english},
	note = {{TLDR}: To address the challenges of false positives and missed detections in face mask detection within dense environments, a feature map convolution approach is introduced to transform the feature map information into the initial weights of convolutional kernels, accelerating the convergence of model training.},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\HE43YWM5\\Wen 等 - 2025 - MF-YOLO Mask Wearing Detection Algorithm for Dense Environments.pdf:application/pdf},
}

@article{ferreira_self-supervised_2025,
	title = {Self-supervised learning for label-free segmentation in cardiac ultrasound},
	volume = {16},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-025-59451-5},
	doi = {10.1038/s41467-025-59451-5},
	abstract = {Abstract
            
              Segmentation and measurement of cardiac chambers from ultrasound is critical, but laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same problematic manual annotations. We build a pipeline for self-supervised segmentation combining computer vision, clinical knowledge, and deep learning. We train on 450 echocardiograms and test on 18,423 echocardiograms (including external data), using the resulting segmentations to calculate measurements. Coefficient of determination (r
              2
              ) between clinically measured and pipeline-predicted measurements (0.55-0.84) are comparable to inter-clinician variation and to supervised learning. Average accuracy for detecting abnormal chambers is 0.85 (0.71-0.97). A subset of test echocardiograms (
              n
               = 553) have corresponding cardiac {MRIs} (the gold standard). Correlation between pipeline and {MRI} measurements is similar to that of clinical echocardiogram. Finally, the pipeline segments the left ventricle with an average Dice score of 0.89 (95\% {CI} [0.89]). Our results demonstrate a manual-label free, clinically valid, and scalable method for segmentation from ultrasound.},
	pages = {4070},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat. Commun.},
	author = {Ferreira, Danielle L. and Lau, Connor and Salaymang, Zaynaf and Arnaout, Rima},
	urldate = {2025-05-22},
	date = {2025-04-30},
	langid = {english},
	note = {{TLDR}: A pipeline for self-supervised segmentation combining computer vision, clinical knowledge, and deep learning is built, demonstrating a manual-label free, clinically valid, and scalable method for segmentation from ultrasound.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\ARFCM9GE\\Ferreira 等 - 2025 - Self-supervised learning for label-free segmentation in cardiac ultrasound.pdf:application/pdf},
}

@article{nezhad_self-supervised_2025,
	title = {Self-supervised learning framework for efficient classification of endoscopic images using pretext tasks},
	volume = {20},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0322028},
	doi = {10.1371/journal.pone.0322028},
	abstract = {Identifying anatomical landmarks in endoscopic video frames is essential for the early diagnosis of gastrointestinal diseases. However, this task remains challenging due to variability in visual characteristics across different regions and the limited availability of annotated data. In this study, we propose a novel self-supervised learning ({SSL}) framework that integrates three complementary pretext task, colorization, jigsaw puzzle solving, and patch prediction, to enhance feature learning from unlabeled endoscopic images. By leveraging these tasks, our model extracts rich, meaningful representations, improving the downstream classification of Z-line, esophageal, and antrum/pylorus regions. To further enhance feature extraction and model interpretability, we incorporate attention mechanisms, transformer-based architectures, and Grad-{CAM} visualization. The integration of attention layers and transformers strengthens the model’s ability to learn discriminative and generalizable features, while Grad-{CAM} improves explainability by highlighting critical decision-making regions. These enhancements make our approach more suitable for clinical deployment, ensuring both high accuracy and interpretability. We evaluate our proposed framework on a comprehensive dataset, demonstrating substantial improvements in classification accuracy, precision, recall, and F1-score compared to conventional models trained without {SSL}. Specifically, our combined model achieves a classification accuracy of 98\%, with high precision and recall across all classes, as reflected in {ROC} curves and confusion matrices. These results underscore the effectiveness of pretext-task-based {SSL}, attention mechanism, and transformers for anatomical landmark identification in endoscopic video frames. Our work introduces a scalable and interpretable methodology for improving endoscopic image classification, reducing reliance on large annotated datasets while enhancing model performance in real-world clinical applications. Future research will explore validation on diverse datasets, real-time diagnostic integration, and scalability to further advance medical image analysis using {SSL}.},
	pages = {e0322028},
	number = {5},
	journaltitle = {{PLOS} One},
	shortjournal = {{PLOS} One},
	author = {Nezhad, Shima Ayyoubi and Tajeddin, Golnaz and Khatibi, Toktam and Sohrabi, Masoudreza},
	editor = {Chu, Lei},
	urldate = {2025-05-22},
	date = {2025-05-08},
	langid = {english},
	note = {{TLDR}: A novel self-supervised learning ({SSL}) framework that integrates three complementary pretext task, colorization, jigsaw puzzle solving, and patch prediction, to enhance feature learning from unlabeled endoscopic images is proposed, introducing a scalable and interpretable methodology for improving endoscopic image classification.},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\4LEDB6DT\\Nezhad 等 - 2025 - Self-supervised learning framework for efficient classification of endoscopic images using pretext t.pdf:application/pdf},
}

@misc{roth_domain-adaptive_2024,
	title = {Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification in Gastrointestinal Endoscopy},
	url = {http://arxiv.org/abs/2410.21302},
	doi = {10.48550/arXiv.2410.21302},
	abstract = {Video capsule endoscopy has transformed gastrointestinal endoscopy ({GIE}) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large {GIE} dataset, called {EndoExtend}24, created by merging ten existing public and private datasets, ensuring patient integrity across splits. {EndoExtend}24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of {GIE} medical image diagnosis. Specifically, the {EVA}-02 model, which is based on the {ViT} architecture and trained on {ImageNet}-22k with masked image modeling (using {EVA}-{CLIP} as a {MIM} teacher), is pre-trained on the {EndoExtend}24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Our model demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge. We achieved a macro {AUC} of 0.762 and a balanced accuracy of 37.1\% on the test set. These results emphasize the effectiveness of our domain-adaptive pre-training approach and the enriched {EndoExtend}24 dataset in advancing gastrointestinal endoscopy diagnostics.},
	number = {{arXiv}:2410.21302},
	publisher = {{arXiv}},
	author = {Roth, Marcel and Nowak, Micha V. and Krenzer, Adrian and Puppe, Frank},
	urldate = {2025-05-22},
	date = {2024-12-11},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {2410.21302 [cs]},
	note = {{TLDR}: This work proposes to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of {GIE} medical image diagnosis, and demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, linter/error},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\9UFZAJGL\\Roth 等 - 2024 - Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification i.pdf:application/pdf;Snapshot:D\:\\Download\\Zotero\\storage\\TNHZMA44\\2410.html:text/html},
}

@article{pathak_context_2016,
	title = {Context Encoders: Feature Learning by Inpainting},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1604.07379},
	doi = {10.48550/ARXIV.1604.07379},
	shorttitle = {Context Encoders},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for {CNN} pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	urldate = {2025-05-22},
	date = {2016},
	langid = {american},
	note = {Publisher: {arXiv}
Version Number: 2
publisher: {arXiv}},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Graphics (cs.{GR}), Machine Learning (cs.{LG})},
	file = {PDF:D\:\\Download\\Zotero\\storage\\AULRRLM9\\Pathak 等 - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf},
}

@misc{zhang_colorful_2016,
	title = {Colorful Image Colorization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1603.08511},
	doi = {10.48550/ARXIV.1603.08511},
	abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a {CNN} at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test," asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32\% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
	publisher = {{arXiv}},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
	urldate = {2025-05-22},
	date = {2016},
	langid = {american},
	note = {Version Number: 5},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, linter/error},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\H3VVZTP8\\Zhang 等 - 2016 - Colorful Image Colorization.pdf:application/pdf},
}

@book{wu_deep_2023,
	location = {Beijing, China},
	title = {Deep Learning Application Development - Practice of {TensorFlow}},
	isbn = {978-7-04-057656-6},
	pagetotal = {415},
	publisher = {Higher Education Press},
	author = {Wu, Minghui and Li, Zhuorong and Jin, Canghong},
	date = {2023-04},
	langid = {english},
	keywords = {linter/error},
}

@misc{rai_surgpose_2025,
	title = {{SurgPose}: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision},
	rights = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2505.11439},
	doi = {10.48550/ARXIV.2505.11439},
	shorttitle = {{SurgPose}},
	abstract = {Accurate pose estimation of surgical tools in Robot-assisted Minimally Invasive Surgery ({RMIS}) is essential for surgical navigation and robot control. While traditional marker-based methods offer accuracy, they face challenges with occlusions, reflections, and tool-specific designs. Similarly, supervised learning methods require extensive training on annotated datasets, limiting their adaptability to new tools. Despite their success in other domains, zero-shot pose estimation models remain unexplored in {RMIS} for pose estimation of surgical instruments, creating a gap in generalising to unseen surgical tools. This paper presents a novel 6 Degrees of Freedom ({DoF}) pose estimation pipeline for surgical instruments, leveraging state-of-the-art zero-shot {RGB}-D models like the {FoundationPose} and {SAM}-6D. We advanced these models by incorporating vision-based depth estimation using the {RAFT}-Stereo method, for robust depth estimation in reflective and textureless environments. Additionally, we enhanced {SAM}-6D by replacing its instance segmentation module, Segment Anything Model ({SAM}), with a fine-tuned Mask R-{CNN}, significantly boosting segmentation accuracy in occluded and complex conditions. Extensive validation reveals that our enhanced {SAM}-6D surpasses {FoundationPose} in zero-shot pose estimation of unseen surgical instruments, setting a new benchmark for zero-shot {RGB}-D pose estimation in {RMIS}. This work enhances the generalisability of pose estimation for unseen objects and pioneers the application of {RGB}-D zero-shot methods in {RMIS}.},
	publisher = {{arXiv}},
	author = {Rai, Utsav and Xu, Haozheng and Giannarou, Stamatia},
	urldate = {2025-05-22},
	date = {2025},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Machine Learning (cs.{LG}), Robotics (cs.{RO}), linter/error},
}

@article{macron_source-free_2025,
	title = {Source-free adaptation for multi-person pose estimation in complex environments},
	author = {Macron, Tolu},
	date = {2025},
	langid = {english},
	file = {Source-Free-Adaptation-for-Multi-Person-Pose-Estimation-in-Complex-Environments:D\:\\Download\\Zotero\\storage\\87D6AU9H\\Source-Free-Adaptation-for-Multi-Person-Pose-Estimation-in-Complex-Environments.pdf:application/pdf},
}

@article{lin_research_2024,
	title = {Research on student classroom behavior detection based on the real-time detection transformer algorithm},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/14/6153},
	doi = {10.3390/app14146153},
	abstract = {With the rapid development of artificial intelligence and big data technology, intelligent education systems have become a key research focus in the field of modern educational technology. This study aims to enhance the intelligence level of educational systems by accurately detecting student behavior in the classroom using deep learning techniques. We propose a method for detecting student classroom behavior based on an improved {RT} {DETR} (Real-Time Detection Transformer) object detection algorithm. By combining actual classroom observation data with {AI}-generated data, we create a comprehensive and diverse student behavior dataset ({FSCB}-dataset). This dataset not only more realistically simulates the classroom environment but also effectively addresses the scarcity of datasets and reduces the cost of dataset construction. The study introduces {MobileNetV}3 as a lightweight backbone network, reducing the model parameters to one-tenth of the original while maintaining nearly the same accuracy. Additionally, by incorporating learnable position encoding and dynamic upsampling techniques, the model significantly improves its ability to recognize small objects and complex scenes. Test results on the {FSCB}-dataset show that the improved model achieves significant improvements in real-time performance and computational efficiency. The lightweight network is also easy to deploy on mobile devices, demonstrating its practicality in resource-constrained environments.},
	pages = {6153},
	number = {14},
	journaltitle = {Applied Sciences},
	shortjournal = {Appl. Sci.},
	author = {Lin, Lihua and Yang, Haodong and Xu, Qingchuan and Xue, Yanan and Li, Dan},
	urldate = {2025-05-23},
	date = {2024-07-15},
	langid = {english},
	note = {Publisher: {MDPI} {AG}
{TLDR}: The study introduces {MobileNetV}3 as a lightweight backbone network and proposes a method for detecting student classroom behavior based on an improved {RT} {DETR} (Real-Time Detection Transformer) object detection algorithm, achieving significant improvements in real-time performance and computational efficiency.},
	file = {全文:D\:\\Download\\Zotero\\storage\\UHMF8R74\\Lin 等 - 2024 - Research on student classroom behavior detection based on the real-time detection transformer algori.pdf:application/pdf},
}

@article{wang_post-secondary_2024,
	title = {Post-secondary classroom teaching quality evaluation using small object detection model},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-56505-4},
	doi = {10.1038/s41598-024-56505-4},
	abstract = {{AbstractThe} classroom video has a complex background and dense targets. This study utilizes small object detection technology to analyze and evaluate students’ behavior in the classroom, aiming to objectively and accurately assess classroom quality. Firstly, noise is removed from the images using a median filter, and the contrast of the images is enhanced through histogram equalization. Label smoothing is applied to reduce the model’s sensitivity to labels. Then, features are extracted from the preprocessed images, and multi-scale feature fusion is employed to enhance semantic expression across multiple scales. Finally, a combination loss function is utilized to improve the accuracy of multi-object recognition tasks. Real-time detection of students’ behaviors in the classroom is performed based on the small object detection model. The average head-up rate in the classroom is calculated, and the quality of teaching is evaluated and analyzed. This study explores the methods and applications of small object detection technology based on actual teaching cases and analyzes and evaluates its effectiveness in evaluating the quality of higher education classroom teaching. The research findings demonstrate the significant importance of small object detection technology in effectively evaluating students’ learning conditions in higher education classrooms, leading to improved teaching quality and personalized education.},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci. Rep.},
	author = {Wang, Rui and Chen, Shaojie and Tian, Gang and Wang, Pengxiang and Ying, Shi},
	urldate = {2025-05-23},
	date = {2024-03-09},
	langid = {english},
	note = {Publisher: Springer Science and Business Media {LLC}
{TLDR}: The research findings demonstrate the significant importance of small object detection technology in effectively evaluating students’ learning conditions in higher education classrooms, leading to improved teaching quality and personalized education.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\CZ5MU4LC\\Wang 等 - 2024 - Post-secondary classroom teaching quality evaluation using small object detection model.pdf:application/pdf},
}

@article{zhu_csb-yolo_2024,
	title = {Csb-yolo: a rapid and efficient real-time algorithm for classroom student behavior detection},
	volume = {21},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	issn = {1861-8200, 1861-8219},
	url = {https://link.springer.com/10.1007/s11554-024-01515-8},
	doi = {10.1007/s11554-024-01515-8},
	shorttitle = {Csb-yolo},
	abstract = {In recent years, the integration of artificial intelligence in education has become key to enhancing the quality of teaching. This study addresses the real-time detection of student behavior in classroom environments by proposing the Classroom Student Behavior {YOLO} ({CSB}-{YOLO}) model. We enhance the model’s multi-scale feature fusion capability using the Bidirectional Feature Pyramid Network ({BiFPN}). Additionally, we have designed a novel Efficient Re-parameterized Detection Head ({ERD} Head) to accelerate the model’s inference speed and introduced Self-Calibrated Convolutions ({SCConv}) to compensate for any potential accuracy loss resulting from lightweight design. To further optimize performance, model pruning and knowledge distillation are utilized to reduce the model size and computational demands while maintaining accuracy. This makes {CSB}-{YOLO} suitable for deployment on low-performance classroom devices while maintaining robust detection capabilities. Tested on the classroom student behavior dataset {SCB}-{DATASET}3, the distilled and pruned {CSB}-{YOLO}, with only 0.72M parameters and 4.3 Giga Floating-point Operations Per Second ({GFLOPs}), maintains high accuracy and exhibits excellent real-time performance, making it particularly suitable for educational environments.},
	number = {4},
	journaltitle = {Journal of Real-Time Image Processing},
	shortjournal = {J. Real-Time Image Process.},
	author = {Zhu, Wenqi and Yang, Zhijun},
	urldate = {2025-05-23},
	date = {2024-08},
	langid = {english},
	note = {Publisher: Springer Science and Business Media {LLC}},
	file = {PDF:D\:\\Download\\Zotero\\storage\\ZTTRPIM8\\Zhu和Yang - 2024 - Csb-yolo a rapid and efficient real-time algorithm for classroom student behavior detection.pdf:application/pdf},
}

@article{wang_slbdetection-net_2025,
	title = {{SLBDetection}-net: towards closed-set and open-set student learning behavior detection in smart classroom of K-12 education},
	volume = {260},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0957-4174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417424022590},
	doi = {10.1016/j.eswa.2024.125392},
	shorttitle = {{SLBDetection}-net},
	pages = {125392},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Syst. Appl.},
	author = {Wang, Zhifeng and Li, Longlong and Zeng, Chunyan and Dong, Shi and Sun, Jianwen},
	urldate = {2025-05-23},
	date = {2025-01},
	langid = {english},
	note = {Publisher: Elsevier {BV}},
	file = {PDF:D\:\\Download\\Zotero\\storage\\44RAPW53\\Wang 等 - 2025 - SLBDetection-net towards closed-set and open-set student learning behavior detection in smart class.pdf:application/pdf},
}

@article{liu_classroom_2025,
	title = {Classroom behavior recognition using computer vision: a systematic review},
	volume = {25},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/2/373},
	doi = {10.3390/s25020373},
	shorttitle = {Classroom behavior recognition using computer vision},
	abstract = {Behavioral computing based on visual cues has become increasingly important, as it can capture and annotate teachers’ and students’ classroom states on a large scale and in real time. However, there is a lack of consensus on the research status and future trends of computer vision-based classroom behavior recognition. The present study conducted a systematic literature review of 80 peer-reviewed journal articles following the Preferred Reporting Items for Systematic Assessment and Meta-Analysis ({PRISMA}) guidelines. Three research questions were addressed concerning goal orientation, recognition techniques, and research challenges. Results showed that: (1) computer vision-supported classroom behavior recognition focused on four categories: physical action, learning engagement, attention, and emotion. Physical actions and learning engagement have been the primary recognition targets; (2) behavioral categorizations have been defined in various ways and lack connections to instructional content and events; (3) existing studies have focused on college students, especially in a natural classical classroom; (4) deep learning was the main recognition method, and the {YOLO} series was applicable for multiple behavioral purposes; (5) moreover, we identified challenges in experimental design, recognition methods, practical applications, and pedagogical research in computer vision. This review will not only inform the recognition and application of computer vision to classroom behavior but also provide insights for future research.},
	pages = {373},
	number = {2},
	journaltitle = {Sensors},
	shortjournal = {Sens.},
	author = {Liu, Qingtang and Jiang, Xinyu and Jiang, Ruyi},
	urldate = {2025-05-23},
	date = {2025-01-10},
	langid = {english},
	note = {Publisher: {MDPI} {AG}
{TLDR}: A systematic literature review of 80 peer-reviewed journal articles following the Preferred Reporting Items for Systematic Assessment and Meta-Analysis ({PRISMA}) guidelines revealed that deep learning was the main recognition method, and the {YOLO} series was applicable for multiple behavioral purposes.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\XRJEQDNQ\\Liu 等 - 2025 - Classroom Behavior Recognition Using Computer Vision A Systematic Review.pdf:application/pdf},
}

@article{cao_yolo-amm_2025,
	title = {{YOLO}-{AMM}: a real-time classroom behavior detection algorithm based on multi-dimensional feature optimization},
	volume = {25},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/4/1142},
	doi = {10.3390/s25041142},
	shorttitle = {Yolo-amm},
	abstract = {Classroom behavior detection is a key task in constructing intelligent educational environments. However, the existing models are still deficient in detail feature capture capability, multi-layer feature correlation, and multi-scale target adaptability, making it challenging to realize high-precision real-time detection in complex scenes. This paper proposes an improved classroom behavior detection algorithm, {YOLO}-{AMM}, to solve these problems. Firstly, we constructed the Adaptive Efficient Feature Fusion ({AEFF}) module to enhance the fusion of semantic information between different features and improve the model’s ability to capture detailed features. Then, we designed a Multidimensional Feature Flow Network ({MFFN}), which fuses multi-dimensional features and enhances the correlation information between features through the multi-scale feature aggregation module and contextual information diffusion mechanism. Finally, we proposed a Multi-Scale Perception and Fusion Detection Head ({MSPF}-Head), which significantly improves the adaptability of the head to different scale targets by introducing multi-scale feature perception, feature interaction, and fusion mechanisms. The experimental results showed that compared with the {YOLOv}8n model, {YOLO}-{AMM} improved the {mAP}0.5 and {mAP}0.5-0.95 by 3.1\% and 4.0\%, significantly improving the detection accuracy. Meanwhile, {YOLO}-{AMM} increased the detection speed ({FPS}) by 12.9 frames per second to 169.1 frames per second, which meets the requirement for real-time detection of classroom behavior.},
	pages = {1142},
	number = {4},
	journaltitle = {Sensors},
	shortjournal = {Sens.},
	author = {Cao, Yi and Cao, Qian and Qian, Chengshan and Chen, Deji},
	urldate = {2025-05-23},
	date = {2025-02-13},
	langid = {english},
	note = {Publisher: {MDPI} {AG}
{TLDR}: An improved classroom behavior detection algorithm, {YOLO}-{AMM}, is proposed, which meets the requirement for real-time detection of classroom behavior, and significantly improves the adaptability of the head to different scale targets by introducing multi-scale feature perception, feature interaction, and fusion mechanisms.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\KL92DTDG\\Cao 等 - 2025 - YOLO-AMM A Real-Time Classroom Behavior Detection Algorithm Based on Multi-Dimensional Feature Opti.pdf:application/pdf},
}

@inproceedings{rombach_high-resolution_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {High-resolution image synthesis with latent diffusion models},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/9878449/},
	doi = {10.1109/cvpr52688.2022.01042},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {10674--10685},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
	urldate = {2025-05-23},
	date = {2022-06},
	langid = {english},
	note = {{TLDR}: These latent diffusion models achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based {DMs}.},
	file = {已提交版本:D\:\\Download\\Zotero\\storage\\9BJKYQ69\\Rombach 等 - 2022 - High-resolution image synthesis with latent diffusion models.pdf:application/pdf},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical text-conditional image generation with {CLIP} latents},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like {CLIP} have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a {CLIP} image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of {CLIP} enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	number = {{arXiv}:2204.06125},
	publisher = {{arXiv}},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	urldate = {2025-05-23},
	date = {2022-04-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2204.06125 [cs]},
	note = {{TLDR}: It is shown that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity, and the joint embedding space of {CLIP} enables language-guided image manipulations in a zero-shot fashion.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\Q2H2SSUL\\Ramesh 等 - 2022 - Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf:application/pdf;Preprint PDF:D\:\\Download\\Zotero\\storage\\J557LTHJ\\Ramesh 等 - 2022 - Hierarchical text-conditional image generation with CLIP latents.pdf:application/pdf;Snapshot:D\:\\Download\\Zotero\\storage\\K2KQVMIZ\\2204.html:text/html},
}

@misc{ke_rethinking_2021,
	title = {Rethinking positional encoding in language pre-training},
	url = {http://arxiv.org/abs/2006.15595},
	doi = {10.48550/arXiv.2006.15595},
	abstract = {In this work, we investigate the positional encoding methods used in language pretraining (e.g., {BERT}) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol [{CLS}] the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called Transformer with Untied Positional Encoding ({TUPE}). In the self-attention module, {TUPE} computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, {TUPE} unties the [{CLS}] symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on {GLUE} benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at https://github.com/guolinke/{TUPE}.},
	number = {{arXiv}:2006.15595},
	publisher = {{arXiv}},
	author = {Ke, Guolin and He, Di and Liu, Tie-Yan},
	urldate = {2025-05-23},
	date = {2021-03-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.15595 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:D\:\\Download\\Zotero\\storage\\675PV528\\Ke 等 - 2021 - Rethinking positional encoding in language pre-training.pdf:application/pdf},
}

@misc{alif_yolov12_2025,
	title = {{YOLOv}12: A Breakdown of the Key Architectural Features},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2502.14740},
	doi = {10.48550/ARXIV.2502.14740},
	shorttitle = {{YOLOv}12},
	abstract = {This paper presents an architectural analysis of {YOLOv}12, a significant advancement in single-stage, real-time object detection building upon the strengths of its predecessors while introducing key improvements. The model incorporates an optimised backbone (R-{ELAN}), 7x7 separable convolutions, and {FlashAttention}-driven area-based attention, improving feature extraction, enhanced efficiency, and robust detections. With multiple model variants, similar to its predecessors, {YOLOv}12 offers scalable solutions for both latency-sensitive and high-accuracy applications. Experimental results manifest consistent gains in mean average precision ({mAP}) and inference speed, making {YOLOv}12 a compelling choice for applications in autonomous systems, security, and real-time analytics. By achieving an optimal balance between computational efficiency and performance, {YOLOv}12 sets a new benchmark for real-time computer vision, facilitating deployment across diverse hardware platforms, from edge devices to high-performance clusters.},
	publisher = {{arXiv}},
	author = {Alif, Mujadded Al Rabbani and Hussain, Muhammad},
	urldate = {2025-05-23},
	date = {2025},
	langid = {american},
	note = {Version Number: 1
{TLDR}: By achieving an optimal balance between computational efficiency and performance, {YOLOv}12 sets a new benchmark for real-time computer vision, facilitating deployment across diverse hardware platforms, from edge devices to high-performance clusters.},
	keywords = {⭐⭐⭐⭐⭐, Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), deep learning architecture, object detection, real-time computer vision},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\QYUN8GBT\\Alif和Hussain - 2025 - YOLOv12 A Breakdown of the Key Architectural Features.pdf:application/pdf},
}

@misc{liang_contrastive_2025,
	title = {Contrastive and variational approaches in self-supervised learning for complex data mining},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2504.04032},
	doi = {10.48550/ARXIV.2504.04032},
	abstract = {Complex data mining has wide application value in many fields, especially in the feature extraction and classification tasks of unlabeled data. This paper proposes an algorithm based on self-supervised learning and verifies its effectiveness through experiments. The study found that in terms of the selection of optimizer and learning rate, the combination of {AdamW} optimizer and 0.002 learning rate performed best in all evaluation indicators, indicating that the adaptive optimization method can improve the performance of the model in complex data mining tasks. In addition, the ablation experiment further analyzed the contribution of each module. The results show that contrastive learning, variational modules, and data augmentation strategies play a key role in the generalization ability and robustness of the model. Through the convergence curve analysis of the loss function, the experiment verifies that the method can converge stably during the training process and effectively avoid serious overfitting. Further experimental results show that the model has strong adaptability on different data sets, can effectively extract high-quality features from unlabeled data, and improves classification accuracy. At the same time, under different data distribution conditions, the method can still maintain high detection accuracy, proving its applicability in complex data environments. This study analyzed the role of self-supervised learning methods in complex data mining through systematic experiments and verified its advantages in improving feature extraction quality, optimizing classification performance, and enhancing model stability},
	publisher = {{arXiv}},
	author = {Liang, Yingbin and Dai, Lu and Shi, Shuo and Dai, Minghao and Du, Junliang and Wang, Haige},
	urldate = {2025-05-24},
	date = {2025},
	langid = {english},
	note = {Version Number: 1
{TLDR}: The study found that in terms of the selection of optimizer and learning rate, the combination of {AdamW} optimizer and 0.002 learning rate performed best in all evaluation indicators, indicating that the adaptive optimization method can improve the performance of the model in complex data mining tasks.},
	keywords = {{FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Machine Learning (cs.{LG})},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\4FP8SWYL\\Liang 等 - 2025 - Contrastive and variational approaches in self-supervised learning for complex data mining.pdf:application/pdf},
}

@misc{xu_generalizable_2025,
	title = {A generalizable 3D framework and model for self-supervised learning in medical imaging},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2501.11755},
	doi = {10.48550/ARXIV.2501.11755},
	abstract = {Current self-supervised learning methods for 3D medical imaging rely on simple pretext formulations and organ- or modality-specific datasets, limiting their generalizability and scalability. We present 3DINO, a cutting-edge {SSL} method adapted to 3D datasets, and use it to pretrain 3DINO-{ViT}: a general-purpose medical imaging model, on an exceptionally large, multimodal, and multi-organ dataset of {\textasciitilde}100,000 3D medical imaging scans from over 10 organs. We validate 3DINO-{ViT} using extensive experiments on numerous medical imaging segmentation and classification tasks. Our results demonstrate that 3DINO-{ViT} generalizes across modalities and organs, including out-of-distribution tasks and datasets, outperforming state-of-the-art methods on the majority of evaluation metrics and labeled dataset sizes. Our 3DINO framework and 3DINO-{ViT} will be made available to enable research on 3D foundation models or further finetuning for a wide range of medical imaging applications.},
	publisher = {{arXiv}},
	author = {Xu, Tony and Hosseini, Sepehr and Anderson, Chris and Rinaldi, Anthony and Krishnan, Rahul G. and Martel, Anne L. and Goubran, Maged},
	urldate = {2025-05-24},
	date = {2025},
	langid = {american},
	note = {Version Number: 1
{TLDR}: The results demonstrate that 3DINO-{ViT} generalizes across modalities and organs, including out-of-distribution tasks and datasets, outperforming state-of-the-art methods on the majority of evaluation metrics and labeled dataset sizes.},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, {FOS}: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.{IV})},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\GUARRNST\\Xu 等 - 2025 - A generalizable 3D framework and model for self-supervised learning in medical imaging.pdf:application/pdf},
}

@article{wang_generative_2025,
	title = {Generative {UI} design with diffusion models: exploring automated interface creation and human-computer interaction},
	rights = {Creative Commons Attribution 4.0 International},
	issn = {2998-8780},
	url = {https://zenodo.org/doi/10.5281/zenodo.15070312},
	doi = {10.5281/ZENODO.15070312},
	shorttitle = {Generative {UI} design with diffusion models},
	author = {Wang, Shixiao and Zhang, Runsheng and Shi, Xue},
	urldate = {2025-05-24},
	date = {2025-03-15},
	langid = {english},
	note = {Publisher: Pinnacle Science Press},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\GEPLBLHQ\\Wang 等 - 2025 - Generative UI design with diffusion models exploring automated interface creation and human-compute.pdf:application/pdf},
}

@article{bischl_openml_2019,
	title = {{OpenML} benchmarking suites},
	journaltitle = {Arxiv:1708.03731v2 [stat.ml]},
	shortjournal = {Arxiv:1708,03731v2 [stat.ml]},
	author = {Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and Rijn, Jan N. van and Vanschoren, Joaquin},
	date = {2019},
	langid = {english},
}

@software{tian_yolov12_2025-1,
	title = {{YOLOv}12: attention-centric real-time object detectors},
	url = {https://github.com/sunsmarterjie/yolov12},
	author = {Tian, Yunjie and Ye, Qixiang and Doermann, David},
	date = {2025},
}

@article{dao_flashattention_2022,
	title = {{FlashAttention}: fast and memory-efficient exact attention with {IO}-awareness},
	volume = {35},
	url = {http://arxiv.org/abs/2205.14135},
	doi = {10.48550/arXiv.2205.14135},
	shorttitle = {{FlashAttention}},
	abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms {IO}-aware -- accounting for reads and writes between levels of {GPU} memory. We propose {FlashAttention}, an {IO}-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between {GPU} high bandwidth memory ({HBM}) and {GPU} on-chip {SRAM}. We analyze the {IO} complexity of {FlashAttention}, showing that it requires fewer {HBM} accesses than standard attention, and is optimal for a range of {SRAM} sizes. We also extend {FlashAttention} to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. {FlashAttention} trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on {BERT}-large (seq. length 512) compared to the {MLPerf} 1.1 training speed record, 3\${\textbackslash}times\$ speedup on {GPT}-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). {FlashAttention} and block-sparse {FlashAttention} enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on {GPT}-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
	pages = {16344--16359},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	urldate = {2025-05-25},
	date = {2022-06-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2205.14135 [cs]},
	note = {remark: 结果为：闪存注意力提升Transformer长序列效率},
	keywords = {⭐⭐⭐⭐⭐, Computer Science - Machine Learning, Approximate Attention, Efficient Transformers, {FlashAttention}},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\8HRXHKLR\\Dao 等 - 2022 - FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf:application/pdf},
}

@article{he_deep_nodate,
	title = {Deep residual learning for image recognition},
	doi = {10.1109/CVPR.2016.90},
	pages = {770--778},
	journaltitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	langid = {english},
	note = {{TLDR}: This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.},
	file = {已提交版本:D\:\\Download\\Zotero\\storage\\UATCWIQS\\He 等 - Deep residual learning for image recognition.pdf:application/pdf},
}

@article{yang_scb-dataset_2023,
	title = {{SCB}-dataset: a dataset for detecting student classroom behavior},
	journaltitle = {Arxiv Preprint Arxiv:2304.02488},
	shortjournal = {Arxiv Prepr. Arxiv:2304,02488},
	author = {Yang, Fan},
	date = {2023},
	langid = {english},
}

@misc{sapkota_improved_2025,
	title = {Improved {YOLOv}12 with {LLM}-generated synthetic data for enhanced apple detection and benchmarking against {YOLOv}11 and {YOLOv}10},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2503.00057},
	doi = {10.48550/ARXIV.2503.00057},
	abstract = {This study evaluated the performance of the {YOLOv}12 object detection model, and compared against the performances {YOLOv}11 and {YOLOv}10 for apple detection in commercial orchards based on the model training completed entirely on synthetic images generated by Large Language Models ({LLMs}). The {YOLOv}12n configuration achieved the highest precision at 0.916, the highest recall at 0.969, and the highest mean Average Precision ({mAP}@50) at 0.978. In comparison, the {YOLOv}11 series was led by {YOLO}11x, which achieved the highest precision at 0.857, recall at 0.85, and {mAP}@50 at 0.91. For the {YOLOv}10 series, {YOLOv}10b and {YOLOv}10l both achieved the highest precision at 0.85, with {YOLOv}10n achieving the highest recall at 0.8 and {mAP}@50 at 0.89. These findings demonstrated that {YOLOv}12, when trained on realistic {LLM}-generated datasets surpassed its predecessors in key performance metrics. The technique also offered a cost-effective solution by reducing the need for extensive manual data collection in the agricultural field. In addition, this study compared the computational efficiency of all versions of {YOLOv}12, v11 and v10, where {YOLOv}11n reported the lowest inference time at 4.7 ms, compared to {YOLOv}12n's 5.6 ms and {YOLOv}10n's 5.9 ms. Although {YOLOv}12 is new and more accurate than {YOLOv}11, and {YOLOv}10, {YOLO}11n still stays the fastest {YOLO} model among {YOLOv}10, {YOLOv}11 and {YOLOv}12 series of models. (Index: {YOLOv}12, {YOLOv}11, {YOLOv}10, {YOLOv}13, {YOLOv}14, {YOLOv}15, {YOLOE}, {YOLO} Object detection)},
	publisher = {{arXiv}},
	author = {Sapkota, Ranjan and Karkee, Manoj},
	urldate = {2025-05-29},
	date = {2025},
	langid = {english},
	note = {Version Number: 2
{TLDR}: It is demonstrated that {YOLOv}12, when trained on realistic {LLM}-generated datasets surpassed its predecessors in key performance metrics, and offered a cost-effective solution by reducing the need for extensive manual data collection in the agricultural field.},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Computation and Language (cs.{CL})},
	file = {PDF:D\:\\Download\\Zotero\\storage\\PMX3MW93\\Sapkota和Karkee - 2025 - Improved YOLOv12 with LLM-generated synthetic data for enhanced apple detection and benchmarking aga.pdf:application/pdf},
}

@article{ma_toward_2025,
	title = {Toward smart ocean monitoring: real-time detection of marine litter using {YOLOv}12 in support of pollution mitigation},
	volume = {217},
	issn = {0025326X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0025326X25006113},
	doi = {10.1016/j.marpolbul.2025.118136},
	shorttitle = {Toward smart ocean monitoring},
	pages = {118136},
	journaltitle = {Marine Pollution Bulletin},
	shortjournal = {Mar. Pollut. Bull.},
	author = {Ma, Jianhua and Zhou, Yongzhang and Zhou, Zimeng and Zhang, Yuqing and He, Luhao},
	urldate = {2025-05-29},
	date = {2025-08},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\PWUGTWY3\\Ma 等 - 2025 - Toward smart ocean monitoring real-time detection of marine litter using YOLOv12 in support of poll.pdf:application/pdf},
}

@misc{sapkota_rf-detr_2025,
	title = {{RF}-{DETR} object detection vs {YOLOv}12 : a study of transformer-based and {CNN}-based architectures for single-class and multi-class greenfruit detection in complex orchard environments under label ambiguity},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2504.13099},
	doi = {10.48550/ARXIV.2504.13099},
	shorttitle = {{RF}-{DETR} object detection vs {YOLOv}12},
	abstract = {This study conducts a detailed comparison of {RF}-{DETR} object detection base model and {YOLOv}12 object detection model configurations for detecting greenfruits in a complex orchard environment marked by label ambiguity, occlusions, and background blending. A custom dataset was developed featuring both single-class (greenfruit) and multi-class (occluded and non-occluded greenfruits) annotations to assess model performance under dynamic real-world conditions. {RF}-{DETR} object detection model, utilizing a {DINOv}2 backbone and deformable attention, excelled in global context modeling, effectively identifying partially occluded or ambiguous greenfruits. In contrast, {YOLOv}12 leveraged {CNN}-based attention for enhanced local feature extraction, optimizing it for computational efficiency and edge deployment. {RF}-{DETR} achieved the highest mean Average Precision ({mAP}50) of 0.9464 in single-class detection, proving its superior ability to localize greenfruits in cluttered scenes. Although {YOLOv}12N recorded the highest {mAP}@50:95 of 0.7620, {RF}-{DETR} consistently outperformed in complex spatial scenarios. For multi-class detection, {RF}-{DETR} led with an {mAP}@50 of 0.8298, showing its capability to differentiate between occluded and non-occluded fruits, while {YOLOv}12L scored highest in {mAP}@50:95 with 0.6622, indicating better classification in detailed occlusion contexts. Training dynamics analysis highlighted {RF}-{DETR}'s swift convergence, particularly in single-class settings where it plateaued within 10 epochs, demonstrating the efficiency of transformer-based architectures in adapting to dynamic visual data. These findings validate {RF}-{DETR}'s effectiveness for precision agricultural applications, with {YOLOv}12 suited for fast-response scenarios. \&gt;Index Terms: {RF}-{DETR} object detection, {YOLOv}12, {YOLOv}13, {YOLOv}14, {YOLOv}15, {YOLOE}, {YOLO} World, {YOLO}, You Only Look Once, Roboflow, Detection Transformers, {CNNs}},
	publisher = {{arXiv}},
	author = {Sapkota, Ranjan and Cheppally, Rahul Harsha and Sharda, Ajay and Karkee, Manoj},
	urldate = {2025-05-29},
	date = {2025},
	langid = {english},
	note = {Version Number: 1
{TLDR}: This study conducts a detailed comparison of {RF}-{DETR} object detection base model and {YOLOv}12 object detection model configurations for detecting greenfruits in a complex orchard environment marked by label ambiguity, occlusions, and background blending to validate {RF}-{DETR}'s effectiveness for precision agricultural applications.},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\IRUI9429\\Sapkota 等 - 2025 - RF-DETR object detection vs YOLOv12  a study of transformer-based and CNN-based architectures for s.pdf:application/pdf},
}

@misc{makary_hybrid_2025,
	title = {A hybrid detection framework for traffic signs using {YOLOv}12, Detectron2, and vision transformers},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	url = {https://www.techrxiv.org/users/916913/articles/1289718-a-hybrid-detection-framework-for-traffic-signs-using-yolov12-detectron2-and-vision-transformers?commit=ab876b6b03dbd48aeeac90a60d3588da7e58b1aa},
	doi = {10.36227/techrxiv.174611340.01774355/v1},
	publisher = {Preprints},
	author = {Makary, Antonio and Dagher, Issam},
	urldate = {2025-05-29},
	date = {2025-05-01},
	langid = {english},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\BDHT48TU\\Makary和Dagher - 2025 - A hybrid detection framework for traffic signs using YOLOv12, Detectron2, and vision transformers.pdf:application/pdf},
}

@article{gao_systematic_2025,
	title = {A systematic survey on human pose estimation: upstream and downstream tasks, approaches, lightweight models, and prospects},
	volume = {58},
	issn = {1573-7462},
	url = {https://link.springer.com/10.1007/s10462-024-11060-2},
	doi = {10.1007/s10462-024-11060-2},
	shorttitle = {A systematic survey on human pose estimation},
	abstract = {In recent years, human pose estimation has been widely studied as a branch task of computer vision. Human pose estimation plays an important role in the development of medicine, fitness, virtual reality, and other fields. Early human pose estimation technology used traditional manual modeling methods. Recently, human pose estimation technology has developed rapidly using deep learning. This study not only reviews the basic research of human pose estimation but also summarizes the latest cutting-edge technologies. In addition to systematically summarizing the human pose estimation technology, this article also extends to the upstream and downstream tasks of human pose estimation, which shows the positioning of human pose estimation technology more intuitively. In particular, considering the issues regarding computer resources and challenges concerning model performance faced by human pose estimation, the lightweight human pose estimation models and the transformer-based human pose estimation models are summarized in this paper. In general, this article classifies human pose estimation technology around types of methods, 2D or 3D representation of outputs, the number of people, views, and temporal information. Meanwhile, classic datasets and targeted datasets are mentioned in this paper, as well as metrics applied to these datasets. Finally, we generalize the current challenges and possible development of human pose estimation technology in the future.},
	pages = {68},
	number = {3},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif. Intell. Rev.},
	author = {Gao, Zheyan and Chen, Jinyan and Liu, Yuxin and Jin, Yucheng and Tian, Dingxiaofei},
	urldate = {2025-05-31},
	date = {2025-01-06},
	langid = {english},
	note = {{TLDR}: The lightweight human pose estimation models and the transformer-based human pose estimation models are summarized in this paper, which classifies human pose estimation technology around types of methods, 2D or 3D representation of outputs, the number of people, views, and temporal information.},
	keywords = {⭐⭐⭐⭐⭐, Dataset},
	file = {PDF:D\:\\Download\\Zotero\\storage\\N6RT5WGK\\Gao 等 - 2025 - A systematic survey on human pose estimation upstream and downstream tasks, approaches, lightweight.pdf:application/pdf},
}

@article{ge_yolo-afr_2025,
	title = {{YOLO}-{AFR}: an improved {YOLOv}12-based model for accurate and real-time dangerous driving behavior detection},
	volume = {15},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/15/11/6090},
	doi = {10.3390/app15116090},
	shorttitle = {Yolo-afr},
	abstract = {Accurate detection of dangerous driving behaviors is crucial for improving the safety of intelligent transportation systems. However, existing methods often struggle with limited feature extraction capabilities and insufficient attention to multiscale and contextual information. To overcome these limitations, we propose {YOLO}-{AFR} ({YOLO} with Adaptive Feature Refinement) for dangerous driving behavior detection. {YOLO}-{AFR} builds upon the {YOLOv}12 architecture and introduces three key innovations: (1) the redesign of the original A2C2f module by introducing a Feature-Refinement Feedback Network ({FRFN}), resulting in a new A2C2f-{FRFN} structure that adaptively refines multiscale features, (2) the integration of self-calibrated convolution ({SC}-Conv) modules in the backbone to enhance multiscale contextual modeling, and (3) the employment of a {SEAM}-based detection head to improve global contextual awareness and prediction accuracy. These three modules combine to form a Calibration-Refinement Loop, which progressively reduces redundancy and enhances discriminative features layer by layer. We evaluate {YOLO}-{AFR} on two public driver behavior datasets, {YawDD}-E and {SfdDD}. Experimental results show that {YOLO}-{AFR} significantly outperforms the baseline {YOLOv}12 model, achieving improvements of 1.3\% and 1.8\% in {mAP}@0.5, and 2.6\% and 12.3\% in {mAP}@0.5:0.95 on the {YawDD}-E and {SfdDD} datasets, respectively, demonstrating its superior performance in complex driving scenarios while maintaining high inference speed.},
	pages = {6090},
	number = {11},
	journaltitle = {Applied Sciences},
	shortjournal = {Appl. Sci.},
	author = {Ge, Tianchen and Ning, Bo and Xie, Yiwu},
	urldate = {2025-06-03},
	date = {2025-05-28},
	langid = {english},
	note = {{TLDR}: 实验结果表明，{YOLO}-{AFR} 的性能明显优于基线 {YOLOv}12 模型，在保持高推理速度的同时，展示了其在复杂驾驶场景中的优越性能。},
	keywords = {Dangerous Driving Behavior Detection, Multiscale Feature Refinement, {YOLO}-{AFR}},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\T66XIEDN\\Ge 等 - 2025 - YOLO-AFR an improved YOLOv12-based model for accurate and real-time dangerous driving behavior dete.pdf:application/pdf},
}

@article{yurish_advances_2025,
	title = {Advances in signal processing and artificial intelligence: proceedings of the 7th international conference on advances in signal processing and artificial intelligence 8-10 april 2025 innsbruck, austria edited by sergey Y. Yurish},
	url = {https://rgdoi.net/10.13140/RG.2.2.14725.46566},
	doi = {10.13140/RG.2.2.14725.46566},
	shorttitle = {Advances in signal processing and artificial intelligence},
	pages = {66--71},
	author = {Yurish, Sergey Y},
	urldate = {2025-06-03},
	date = {2025},
	langid = {english},
	note = {Publisher: Unpublished},
	keywords = {Artificial Intelligence, Research Proposal, Signal Processing},
	file = {PDF:D\:\\Download\\Zotero\\storage\\LBCU7MCN\\Yurish - 2025 - Advances in Signal Processing and Artificial Intelligence Proceedings of the 7th International Conf.pdf:application/pdf},
}

@misc{cordonnier_relationship_2019,
	title = {On the relationship between self-attention and convolutional layers},
	rights = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/1911.03584},
	doi = {10.48550/ARXIV.1911.03584},
	abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping {CNNs} to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to {CNN} layers, corroborating our analysis. Our code is publicly available.},
	publisher = {{arXiv}},
	author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
	urldate = {2025-06-06},
	date = {2019},
	langid = {english},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Computation and Language (cs.{CL}), Machine Learning (stat.{ML})},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\MDZYNRV6\\Cordonnier 等 - 2019 - On the relationship between self-attention and convolutional layers.pdf:application/pdf},
}

@inproceedings{wang_yolov7_2023,
	location = {Vancouver, {BC}, Canada},
	title = {{YOLOv}7: trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0129-8},
	url = {https://ieeexplore.ieee.org/document/10204762/},
	doi = {10.1109/CVPR52729.2023.00721},
	shorttitle = {{YOLOv}7},
	abstract = {Real-time object detection is one of the most important research topics in computer vision. As new approaches regarding architecture optimization and training optimization are continually being developed, we have found two research topics that have spawned when dealing with these latest state-of-the-art methods. To address the topics, we propose a trainable bag-of-freebies oriented solution. We combine the ﬂexible and efﬁcient training tools with the proposed architecture and the compound scaling method. {YOLOv}7 surpasses all known object detectors in both speed and accuracy in the range from 5 {FPS} to 120 {FPS} and has the highest accuracy 56.8\% {AP} among all known realtime object detectors with 30 {FPS} or higher on {GPU} V100. Source code is released in https://github.com/ {WongKinYiu}/yolov7.},
	eventtitle = {2023 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {7464--7475},
	booktitle = {2023 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	urldate = {2025-06-07},
	date = {2023-06},
	langid = {english},
	note = {{TLDR}: {YOLOv}7 surpasses all known object detectors in both speed and accuracy in the range from 5 {FPS} to 120 {FPS} and has the highest accuracy 56.8\% {AP} among all known real-time object detectors with 30 {FPS} or higher on {GPU} V100.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\8HSGQLG3\\Wang 等 - 2023 - YOLOv7 Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors.pdf:application/pdf},
}

@article{yin_mas-yolo_2025,
	title = {{MAS}-{YOLO}: a lightweight detection algorithm for {PCB} defect detection based on improved {YOLOv}12},
	volume = {15},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/15/11/6238},
	doi = {10.3390/app15116238},
	shorttitle = {Mas-yolo},
	abstract = {As the performance requirements for printed circuit boards ({PCBs}) in electronic devices continue to increase, reliable defect detection during {PCB} manufacturing is vital. However, due to the small size, complex categories, and subtle differences in defect features, traditional detection methods are limited in accuracy and robustness. To overcome these challenges, this paper proposes {MAS}-{YOLO}, a lightweight detection algorithm for {PCB} defect detection based on improved {YOLOv}12 architecture. In the Backbone, a Medianenhanced Channel and Spatial Attention Block ({MECS}) expands the receptive field through median enhancement and depthwise convolution to generate attention maps that effectively capture subtle defect features. In the Neck, an Adaptive Hierarchical Feature Integration Network ({AHFIN}) adaptively fuses multi-scale features through weighted integration, enhancing feature utilization and focus on defect regions. Moreover, the original {YOLOv}12 loss function is replaced with the Slide Alignment Loss ({SAL}) to improve bounding box localization and detect complex defect types. Experimental results demonstrate that {MASYOLO} significantly improves mean average precision ({mAP}) and frames per second ({FPS}) compared to the original {YOLOv}12, fulfilling real-time industrial detection requirements.},
	pages = {6238},
	number = {11},
	journaltitle = {Applied Sciences},
	shortjournal = {Appl. Sci.},
	author = {Yin, Xupeng and Zhao, Zikai and Weng, Liguo},
	urldate = {2025-06-09},
	date = {2025-06-01},
	langid = {english},
	note = {{TLDR}: Experimental results demonstrate that {MAS}-{YOLO} significantly improves mean average precision ({mAP}) and frames per second ({FPS}) compared to the original {YOLOv}12, fulfilling real-time industrial detection requirements.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\VMPZSZEC\\Yin 等 - 2025 - MAS-YOLO A Lightweight Detection Algorithm for PCB Defect Detection Based on Improved YOLOv12.pdf:application/pdf},
}

@article{chileshe_early_2025,
	title = {Early detection of sexually transmitted infections using {YOLO} 12: a deep learning approach},
	volume = {15},
	issn = {2165-3917, 2165-3925},
	url = {https://www.scirp.org/journal/doi.aspx?doi=10.4236/ojapps.2025.154078},
	doi = {10.4236/ojapps.2025.154078},
	shorttitle = {Early detection of sexually transmitted infections using {YOLO} 12},
	abstract = {This paper focuses on the use of {YOLOv}12 for the early detection of Sexually Transmitted Infections, which are a global public health challenge. {YOLOv}12 is a deep-learning model released on February 18th, 2025. Its release has shifted from the traditional {CNN}-based approaches to attention-centric architecture yet still maintains high accuracy, fast inference and robust object detection capabilities with better global context modeling. This raises many interesting questions, such as whether it can perform better on real-world problems such as early detection {STIs}. Can the model show consistent results on different skin tones? Can it help reduce the risk of long-term effects of untreated {STIs}? Can {YOLOv}12 outperform {YOLOv}10 and {YOLOv}11? How can we validate the results? This study will answer these questions and show us how we arrived at our conclusions.},
	pages = {1126--1144},
	number = {4},
	journaltitle = {Open Journal of Applied Sciences},
	shortjournal = {Open J. Appl. Sci.},
	author = {Chileshe, Martin and Nyirenda, Mayumbo and Kaoma, John},
	urldate = {2025-06-09},
	date = {2025},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\DEHSXJ2F\\Chileshe 等 - 2025 - Early Detection of Sexually Transmitted Infections Using YOLO 12 A Deep Learning Approach.pdf:application/pdf},
}

@article{he_intelligent_2025,
	title = {An intelligent measurement system for multi-type rock detection based on {YOLOv}12 deep learning framework},
	doi = {10.2139/ssrn.5280851},
	author = {He, Luhao and Zhou, Yongzhang and Liu, Guoqing and Ma, Jianhua},
	date = {2025},
	langid = {english},
	keywords = {{YOLOv}12, Deep Learning, Rock Detection},
	file = {PDF:D\:\\Download\\Zotero\\storage\\CNNWZ37T\\He 等 - An Intelligent Measurement System for Multi-Type Rock Detection Based on YOLOv12 Deep Learning Frame.pdf:application/pdf},
}

@incollection{mallick_addressing_2020,
	location = {Singapore},
	title = {Addressing the false positives in pedestrian detection},
	volume = {686},
	isbn = {978-981-15-7030-8 978-981-15-7031-5},
	url = {http://link.springer.com/10.1007/978-981-15-7031-5_103},
	pages = {1083--1092},
	booktitle = {Electronic Systems and Intelligent Computing},
	publisher = {Springer Singapore},
	author = {Karthika, N. J. and Chandran, Saravanan},
	editor = {Mallick, Pradeep Kumar and Meher, Preetisudha and Majumder, Alak and Das, Santos Kumar},
	urldate = {2025-06-11},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-981-15-7031-5_103},
	note = {Series Title: Lecture Notes in Electrical Engineering},
	keywords = {Pedestrian Detection Data set},
}

@inproceedings{zhang_crowd_2018,
	title = {Crowd counting via scale-adaptive convolutional neural network},
	doi = {10.1109/WACV.2018.00127},
	pages = {1113--1121},
	booktitle = {2018 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
	publisher = {{IEEE}},
	author = {Zhang, Lu and Shi, Miaojing and Chen, Qiaobo},
	date = {2018},
	langid = {english},
	note = {{TLDR}: A scale-adaptive {CNN} ({SaCNN}) architecture with a backbone of fixed small receptive fields is proposed to improve the network generalization on crowd scenes with few pedestrians, where most representative approaches perform poorly on.},
	keywords = {{OpenDataLab}\_SmartCity},
}

@misc{noauthor_crowd_2021,
	title = {Crowd Pose},
	url = {https://www.kaggle.com/datasets/elmahy/crowdpose},
	version = {Version 12},
	author = {, Elmahy},
	urldate = {2025-06-11},
	date = {2021-03-15},
	keywords = {Crowd Pose},
}

@article{ting_material_2024,
	title = {Material classification via embedded {RF} antenna array and machine learning for intelligent mobile robots},
	volume = {106},
	issn = {11100168},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1110016824006926},
	doi = {10.1016/j.aej.2024.06.083},
	abstract = {In this work, we present a novel design for an embedded Radio Frequency ({RF}) antenna array that can distinguish various materials by analyzing changes in Received Signal Strength Indicator ({RSSI}) values. The use of a low-cost and small-form-factor microcontroller by Espressif makes this design both cost-effective and suitable for integration into various applications, differentiating it from previous studies. To enhance the material classification performance, a combination of Kalman filter and Support Vector Machine is proposed which does not require a large amount of training data for model optimization. Results demonstrate that the proposed machine learning model is able to perform material classification within a 2 m range, with an average accuracy of over 96\%. Such a system is well-suited for intelligent mobile robotic applications particularly in warehouse automation or smart manufacturing lines due to its ability for proximal remote sensing, real-time monitoring, and multimodal sensing.},
	pages = {60--70},
	journaltitle = {Alexandria Engineering Journal},
	shortjournal = {Alexandria Eng. J.},
	author = {Ting, Te Meng and Ahmad, Nur Syazreen and Goh, Patrick},
	urldate = {2025-06-12},
	date = {2024-11},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\5693HHNP\\Ting 等 - 2024 - Material classification via embedded RF antenna array and machine learning for intelligent mobile ro.pdf:application/pdf},
}

@article{ribeiro_instance_nodate,
	title = {Instance segmentation in medical imaging: a comparative study of {CNN} and transformer-based models in a teledermatology study-case},
	doi = {10.5753/sbcas.2025.7814},
	abstract = {The rapid evolution of instance segmentation models necessitates empirical comparisons to guide their adoption in critical domains like medical imaging. This study evaluates four state-of-the-art architectures—Mask R-{CNN}, Mask2Former, {YOLOv}11, and {YOLOv}12 on a teledermatological dataset annotated for compliance-driven segmentation of rulers and patient information tags. Results demonstrate that transformer-based and hybrid models (Mask2Former, {YOLOv}11) significantly outperform traditional {CNNs} in precision-driven metrics ({AP}75), highlighting their suitability for medical applications. This work provides actionable insights for model selection in healthcare, emphasizing the balance between accuracy and computational efficiency.},
	author = {Ribeiro, Rodrigo P S},
	langid = {english},
	note = {{TLDR}: This study evaluates four state-of-the-art architectures—Mask R-{CNN}, Mask2Former, {YOLOv}11, and {YOLOv}12 on a teledermatological dataset annotated for compliance-driven segmentation of rulers and patient information tags to demonstrate their suitability for medical applications.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\6JTPWIGJ\\Ribeiro - Instance Segmentation in Medical Imaging A Comparative Study of CNN and Transformer-Based Models in.pdf:application/pdf},
}

@misc{jaswal_exercise_2024,
	title = {Exercise Detection dataset},
	url = {https://www.kaggle.com/datasets/mrigaankjaswal/exercise-detection-dataset},
	version = {Version 1},
	author = {Jaswal, Mrigaank},
	date = {2024-09-22},
	keywords = {Exercise Detection dataset},
}

@misc{noauthor_pedestrian_2019,
	title = {Pedestrian Dataset},
	url = {https://www.kaggle.com/datasets/smeschke/pedestrian-dataset},
	author = {, smeschke},
	date = {2019-09-25},
	keywords = {Pedestrian Dataset},
}

@article{ecarnot_writing_2015,
	title = {Writing a scientific article: a step-by-step guide for beginners},
	volume = {6},
	issn = {18787649},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1878764915001606},
	doi = {10.1016/j.eurger.2015.08.005},
	shorttitle = {Writing a scientific article},
	pages = {573--579},
	number = {6},
	journaltitle = {European Geriatric Medicine},
	shortjournal = {Eur. Geriatr. Med.},
	author = {Ecarnot, F. and Seronde, M.-F. and Chopard, R. and Schiele, F. and Meneveau, N.},
	urldate = {2025-06-18},
	date = {2015-12},
	langid = {english},
	note = {{TLDR}: The basic steps to follow in writing a scientific article are described, which outline the main sections that an average article should contain; the elements that should appear in these sections, and some pointers for making the overall result attractive and acceptable for publication.},
	file = {全文:D\:\\Download\\Zotero\\storage\\24BAF7KN\\Ecarnot 等 - 2015 - Writing a scientific article a step-by-step guide for beginners.pdf:application/pdf},
}

@article{li_survey_2021,
	title = {A survey of convolutional neural networks: analysis, applications, and prospects},
	volume = {33},
	doi = {10.1109/TNNLS.2021.3084827},
	pages = {6999--7019},
	number = {12},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learn. Syst.},
	author = {Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
	date = {2021},
	langid = {english},
	note = {Publisher: {IEEE}
{TLDR}: This review introduces the history of {CNN}, some classic and advanced {CNN} models are introduced, and an overview of various convolutions is provided, including those key points making them reach state-of-the-art results.},
}

@article{hinton_reducing_2006,
	title = {Reducing the dimensionality of data with neural networks},
	volume = {313},
	doi = {10.1126/science.1127647},
	pages = {504--507},
	number = {5786},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
	date = {2006},
	langid = {english},
	note = {Publisher: American Association for the Advancement of Science},
}

@misc{alom_history_2018,
	title = {The history began from {AlexNet}: a comprehensive survey on deep learning approaches},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/1803.01164},
	doi = {10.48550/ARXIV.1803.01164},
	shorttitle = {The history began from {AlexNet}},
	abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing ({NLP}), Cyber security, and many more. This report presents a brief survey on development of {DL} approaches, including Deep Neural Network ({DNN}), Convolutional Neural Network ({CNN}), Recurrent Neural Network ({RNN}) including Long Short Term Memory ({LSTM}) and Gated Recurrent Units ({GRU}), Auto-Encoder ({AE}), Deep Belief Network ({DBN}), Generative Adversarial Network ({GAN}), and Deep Reinforcement Learning ({DRL}). In addition, we have included recent development of proposed advanced variant {DL} techniques based on the mentioned {DL} approaches. Furthermore, {DL} approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, {SDKs}, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on {RL} [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
	publisher = {{arXiv}},
	author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Van Esesn, Brian C and Awwal, Abdul A S. and Asari, Vijayan K.},
	urldate = {2025-06-19},
	date = {2018},
	langid = {english},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@article{yu_yesterday_2013,
	title = {Yesterday, today and tomorrow of deep learning [J]},
	volume = {50},
	pages = {1799--1804},
	number = {9},
	journaltitle = {Computer Research and Development},
	shortjournal = {Comput. Res. Dev.},
	author = {Yu, Kai and Jia, Lei and Chen, Yuqiang and Xu, Wei},
	date = {2013},
	langid = {english},
}

@article{sandeep_imagenet_2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {2},
	number = {1},
	journaltitle = {International Journal of Artificial Intelligence and Machine Learning},
	shortjournal = {Int. J. Artif. Intell. Mach. Learn.},
	author = {Sandeep, Rahul},
	date = {2012},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\U9E5QMTD\\Sandeep - 2012 - ImageNet classification with deep convolutional neural networks.pdf:application/pdf},
}

@article{sengupta_going_2019,
	title = {Going deeper in spiking neural networks: {VGG} and residual architectures},
	volume = {13},
	doi = {10.3389/fnins.2019.00095},
	pages = {95},
	journaltitle = {Frontiers in Neuroscience},
	shortjournal = {Front. Neurosci.},
	author = {Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu, Chiao and Roy, Kaushik},
	date = {2019},
	langid = {english},
	note = {Publisher: Frontiers Media {SA}
{TLDR}: A novel algorithmic technique is proposed for generating an {SNN} with a deep architecture with significantly better accuracy than the state-of-the-art, and its effectiveness on complex visual recognition problems such as {CIFAR}-10 and {ImageNet} is demonstrated.},
}

@inproceedings{yu_visualizing_2016,
	title = {Visualizing and comparing {AlexNet} and {VGG} using deconvolutional layers},
	volume = {3},
	pages = {43--76},
	booktitle = {Proceedings of the 33 Rd International Conference on Machine Learning},
	author = {Yu, Wei and Yang, Kuiyuan and Bai, Yalong and Xiao, Tianjun and Yao, Hongxun and Rui, Yong},
	date = {2016},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{ballester_performance_2016,
	title = {On the performance of {GoogLeNet} and {AlexNet} applied to sketches},
	volume = {30},
	doi = {10.1609/aaai.v30i1.10171},
	booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Ballester, Pedro and Araujo, Ricardo},
	date = {2016},
	langid = {english},
	note = {Issue: 1
{TLDR}: It is shown that both {GoogLeNet} and {AlexNet} networks are largely unable to recognize abstract sketches that are easily recognizable by humans.},
}

@inproceedings{yuan_feature_2016,
	title = {Feature extraction and image retrieval based on {AlexNet}},
	volume = {10033},
	pages = {65--69},
	booktitle = {Eighth International Conference on Digital Image Processing ({ICDIP} 2016)},
	publisher = {{SPIE}},
	author = {Yuan, Zheng-Wu and Zhang, Jun},
	date = {2016},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{zhang_residual_2017,
	title = {Residual networks of residual networks: multilevel residual networks},
	volume = {28},
	doi = {10.1109/TCSVT.2017.2654543},
	pages = {1303--1314},
	number = {6},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	shortjournal = {{IEEE} Trans. Circuits Syst. Video Technol.},
	author = {Zhang, Ke and Sun, Miao and Han, Tony X and Yuan, Xingfang and Guo, Liru and Liu, Tao},
	date = {2017},
	langid = {english},
	note = {Publisher: {IEEE}
{TLDR}: A novel residual network architecture, residual networks of residual networks ({RoR}) is proposed, to dig the optimization ability of residual Networks, where {RoR} substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping.},
}

@inproceedings{girshick_r-cnn_2014,
	title = {R-cnn for object detection},
	booktitle = {{IEEE} Conference},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra and Mercan, Ezgi},
	date = {2014},
	langid = {english},
}

@inproceedings{xie_oriented_2021,
	title = {Oriented R-{CNN} for object detection},
	pages = {3520--3529},
	booktitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	author = {Xie, Xingxing and Cheng, Gong and Wang, Jiabao and Yao, Xiwen and Han, Junwei},
	date = {2021},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{girshick_fast_2015,
	title = {Fast r-cnn in proceedings of the ieee international conference on computer vision (pp. 1440–1448)},
	volume = {2},
	journaltitle = {Piscataway, {NJ}: Ieee.[google Scholar]},
	shortjournal = {Piscataw. {NJ}: Ieee,[google Sch.},
	author = {Girshick, Ross},
	date = {2015},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@misc{redmon_yolo9000_2016,
	title = {{YOLO}9000: better, faster, stronger},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1612.08242},
	doi = {10.48550/ARXIV.1612.08242},
	shorttitle = {Yolo9000},
	abstract = {We introduce {YOLO}9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the {YOLO} detection method, both novel and drawn from prior work. The improved model, {YOLOv}2, is state-of-the-art on standard detection tasks like {PASCAL} {VOC} and {COCO}. At 67 {FPS}, {YOLOv}2 gets 76.8 {mAP} on {VOC} 2007. At 40 {FPS}, {YOLOv}2 gets 78.6 {mAP}, outperforming state-of-the-art methods like Faster {RCNN} with {ResNet} and {SSD} while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train {YOLO}9000 simultaneously on the {COCO} detection dataset and the {ImageNet} classification dataset. Our joint training allows {YOLO}9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the {ImageNet} detection task. {YOLO}9000 gets 19.7 {mAP} on the {ImageNet} detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in {COCO}, {YOLO}9000 gets 16.0 {mAP}. But {YOLO} can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	publisher = {{arXiv}},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2025-06-19},
	date = {2016},
	langid = {english},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{bochkovskiy_yolov4_2020,
	title = {{YOLOv}4: optimal speed and accuracy of object detection},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2004.10934},
	doi = {10.48550/ARXIV.2004.10934},
	shorttitle = {{YOLOv}4},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network ({CNN}) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections ({WRC}), Cross-Stage-Partial-connections ({CSP}), Cross mini-Batch Normalization ({CmBN}), Self-adversarial-training ({SAT}) and Mish-activation. We use new features: {WRC}, {CSP}, {CmBN}, {SAT}, Mish activation, Mosaic data augmentation, {CmBN}, {DropBlock} regularization, and {CIoU} loss, and combine some of them to achieve state-of-the-art results: 43.5\% {AP} (65.7\% {AP}50) for the {MS} {COCO} dataset at a realtime speed of {\textasciitilde}65 {FPS} on Tesla V100. Source code is at https://github.com/{AlexeyAB}/darknet},
	publisher = {{arXiv}},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	urldate = {2025-06-19},
	date = {2020},
	langid = {english},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, {FOS}: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.{IV})},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\VIYDZSXX\\Bochkovskiy 等 - 2020 - YOLOv4 optimal speed and accuracy of object detection.pdf:application/pdf},
}

@inproceedings{mohod_yolov4_2022,
	title = {Yolov4 vs yolov5: object detection on surveillance videos},
	pages = {654--665},
	booktitle = {International Conference on Advanced Network Technologies and Intelligent Computing},
	publisher = {Springer},
	author = {Mohod, Nikita and Agrawal, Prateek and Madaan, Vishu},
	date = {2022},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{jiang_real-time_2020,
	title = {Real-time object detection method based on improved {YOLOv}4-tiny},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2011.04244},
	doi = {10.48550/ARXIV.2011.04244},
	abstract = {The "You only look once v4"({YOLOv}4) is one type of object detection methods in deep learning. {YOLOv}4-tiny is proposed based on {YOLOv}4 to simple the network structure and reduce parameters, which makes it be suitable for developing on the mobile and embedded devices. To improve the real-time of object detection, a fast object detection method is proposed based on {YOLOv}4-tiny. It firstly uses two {ResBlock}-D modules in {ResNet}-D network instead of two {CSPBlock} modules in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs an auxiliary residual network block to extract more feature information of object to reduce detection error. In the design of auxiliary network, two consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract global features, and channel attention and spatial attention are also used to extract more effective information. In the end, it merges the auxiliary network and backbone network to construct the whole network structure of improved {YOLOv}4-tiny. Simulation results show that the proposed method has faster object detection than {YOLOv}4-tiny and {YOLOv}3-tiny, and almost the same mean value of average precision as the {YOLOv}4-tiny. It is more suitable for real-time object detection.},
	author = {Jiang, Zicong and Zhao, Liquan and Li, Shuaiyang and Jia, Yanfei},
	urldate = {2025-06-19},
	date = {2020},
	langid = {english},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI})},
}

@article{jocher_ultralyticsyolov5_2020,
	title = {ultralytics/yolov5: v3. 0},
	journaltitle = {Zenodo},
	shortjournal = {Zenodo},
	author = {Jocher, Glenn and Stoken, Alex and Borovec, Jirka and Changyu, Liu and Hogan, Adam and Diaconu, Laurentiu and Poznanski, Jake and Yu, Lijun and Rai, Prashant and Ferriday, Russ and {Others}},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{li_yolov6_2022,
	title = {{YOLOv}6: a single-stage object detection framework for industrial applications},
	journaltitle = {Arxiv Preprint Arxiv:2209.02976},
	shortjournal = {Arxiv Prepr. Arxiv:2209,02976},
	author = {Li, Chuyi and Li, Lulu and Jiang, Hongliang and Weng, Kaiheng and Geng, Yifei and Li, Liang and Ke, Zaidan and Li, Qingyuan and Cheng, Meng and Nie, Weiqiang and {Others}},
	date = {2022},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{hussain_yolo-v1_2023,
	title = {{YOLO}-v1 to {YOLO}-v8, the rise of {YOLO} and its complementary nature toward digital manufacturing and industrial defect detection},
	volume = {11},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/11/7/677},
	doi = {10.3390/machines11070677},
	abstract = {Since its inception in 2015, the {YOLO} (You Only Look Once) variant of object detectors has rapidly grown, with the latest release of {YOLO}-v8 in January 2023. {YOLO} variants are underpinned by the principle of real-time and high-classification performance, based on limited but efficient computational parameters. This principle has been found within the {DNA} of all {YOLO} variants with increasing intensity, as the variants evolve addressing the requirements of automated quality inspection within the industrial surface defect detection domain, such as the need for fast detection, high accuracy, and deployment onto constrained edge devices. This paper is the first to provide an in-depth review of the {YOLO} evolution from the original {YOLO} to the recent release ({YOLO}-v8) from the perspective of industrial manufacturing. The review explores the key architectural advancements proposed at each iteration, followed by examples of industrial deployment for surface defect detection endorsing its compatibility with industrial requirements.},
	pages = {677},
	number = {7},
	journaltitle = {Machines},
	shortjournal = {Machines},
	author = {Hussain, Muhammad},
	urldate = {2025-06-20},
	date = {2023-06-23},
	langid = {english},
	note = {{TLDR}: This paper is the first to provide an in-depth review of the Y {OLO} evolution from the original {YOLO} to the recent release ({YOLO}-v8) from the perspective of industrial manufacturing.},
	file = {全文:D\:\\Download\\Zotero\\storage\\35MJIN28\\Hussain - 2023 - YOLO-v1 to YOLO-v8, the rise of YOLO and its complementary nature toward digital manufacturing and i.pdf:application/pdf},
}

@inproceedings{wang_yolov9_2024,
	title = {Yolov9: learning what you want to learn using programmable gradient information},
	pages = {1--21},
	booktitle = {European Conference on Computer Vision},
	publisher = {Springer},
	author = {Wang, Chien-Yao and Yeh, I-Hau and Mark Liao, Hong-Yuan},
	date = {2024},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{wang_yolov10_2024,
	title = {Yolov10: real-time end-to-end object detection},
	volume = {37},
	pages = {107984--108011},
	journaltitle = {Advances in Neural Information Processing Systems},
	shortjournal = {Adv. Neural Inf. Process. Syst.},
	author = {Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and {Others}},
	date = {2024},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@misc{sapkota_comparing_2025,
	title = {Comparing {YOLOv}11 and {YOLOv}8 for instance segmentation of occluded and non-occluded immature green fruits in complex orchard environment},
	url = {http://arxiv.org/abs/2410.19869},
	doi = {10.48550/arXiv.2410.19869},
	abstract = {This study conducted a comprehensive performance evaluation on {YOLO}11 (or {YOLOv}11) and {YOLOv}8, the latest in the "You Only Look Once" ({YOLO}) series, focusing on their instance segmentation capabilities for immature green apples in orchard environments. {YOLO}11n-seg achieved the highest mask precision across all categories with a notable score of 0.831, highlighting its effectiveness in fruit detection. {YOLO}11m-seg and {YOLO}11l-seg excelled in non-occluded and occluded fruitlet segmentation with scores of 0.851 and 0.829, respectively. Additionally, {YOLOv}11x-seg led in mask recall for all categories, achieving a score of 0.815, with {YOLO}11m-seg performing best for non-occluded immature green fruitlets at 0.858 and {YOLOv}8x-seg leading the occluded category with 0.800. In terms of mean average precision at a 50{\textbackslash}\% intersection over union ({mAP}@50), {YOLOv}11m-seg consistently outperformed, registering the highest scores for both box and mask segmentation, at 0.876 and 0.860 for the "All" class and 0.908 and 0.909 for non-occluded immature fruitlets, respectively. {YOLO}11l-seg and {YOLOv}8l-seg shared the top box {mAP}@50 for occluded immature fruitlets at 0.847, while {YOLO}11m-seg achieved the highest mask {mAP}@50 of 0.810. Despite the advancements in {YOLO}11, {YOLOv}8n surpassed its counterparts in image processing speed, with an impressive inference speed of 3.3 milliseconds, compared to the fastest {YOLO}11 series model at 4.8 milliseconds, underscoring its suitability for real-time agricultural applications related to complex green fruit environments. ({YOLOv}11 segmentation)},
	number = {{arXiv}:2410.19869},
	publisher = {{arXiv}},
	author = {Sapkota, Ranjan and Karkee, Manoj},
	urldate = {2025-06-20},
	date = {2025-01-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2410.19869 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\3RIHIN4K\\Sapkota和Karkee - 2025 - Comparing YOLOv11 and YOLOv8 for instance segmentation of occluded and non-occluded immature green f.pdf:application/pdf},
}

@article{wang_linformer_2020,
	title = {Linformer: self-attention with linear complexity},
	journaltitle = {Arxiv Preprint Arxiv:2006.04768},
	shortjournal = {Arxiv Prepr. Arxiv:2006,04768},
	author = {Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{qiao_real-time_2017,
	title = {Real-time human gesture grading based on {OpenPose}},
	doi = {10.1109/CISP-BMEI.2017.8301910},
	pages = {1--6},
	booktitle = {2017 10th International Congress on Image and Signal Processing, Biomedical Engineering and Informatics ({CISP}-{BMEI})},
	publisher = {{IEEE}},
	author = {Qiao, Sen and Wang, Yilin and Li, Jian},
	date = {2017},
	langid = {english},
	note = {{TLDR}: Experimental results showed that the real-time 2D human gesture grading system from monocular images based on {OpenPose} worked efficiently with high real- time performance, low cost of equipment and strong robustness to the interference of noise.},
}

@article{cao_openpose_2019,
	title = {Openpose: realtime multi-person 2d pose estimation using part affinity fields},
	volume = {43},
	doi = {10.1109/TPAMI.2019.2929257},
	pages = {172--186},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	date = {2019},
	langid = {english},
	note = {Publisher: {IEEE}
{TLDR}: {OpenPose} is released, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints, and the first combined body and foot keypoint detector, based on an internal annotated foot dataset.},
}

@article{meng_yolov10-pose_2025,
	title = {{YOLOv}10-pose and {YOLOv}9-pose: real-time strawberry stalk pose detection models},
	volume = {165},
	doi = {10.1016/j.compind.2024.104231},
	pages = {104231},
	journaltitle = {Computers in Industry},
	author = {Meng, Zhichao and Du, Xiaoqiang and Sapkota, Ranjan and Ma, Zenghong and Cheng, Hongchao},
	date = {2025},
	langid = {english},
	note = {Publisher: Elsevier},
}

@article{jafarzadeh_enhancing_2025,
	title = {Enhancing hurdles athletes’ performance analysis: a comparative study of cnn-based pose estimation frameworks},
	pages = {1--19},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimedia Tools Appl.},
	author = {Jafarzadeh, Pouya and Zelioli, Luca and Virjonen, Petra and Farahnakian, Fahimeh and Nevalainen, Paavo and Heikkonen, Jukka},
	date = {2025},
	langid = {english},
	note = {Publisher: Springer},
	file = {PDF:D\:\\Download\\Zotero\\storage\\HVYIV5SG\\Jafarzadeh 等 - 2025 - Enhancing hurdles athletes’ performance analysis a comparative study of cnn-based pose estimation f.pdf:application/pdf},
}

@article{jiang_optimization_2025,
	title = {Optimization study of badminton sports training system based on {MoileNet} {OpenPose} lightweight human posture estimation model},
	doi = {10.1016/j.entcom.2025.100975},
	pages = {100975},
	journaltitle = {Entertainment Computing},
	shortjournal = {Entertain. Comput.},
	author = {Jiang, Kailai},
	date = {2025},
	langid = {english},
	note = {Publisher: Elsevier},
}

@misc{lugaresi_mediapipe_2019,
	title = {{MediaPipe}: a framework for building perception pipelines},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1906.08172},
	doi = {10.48550/ARXIV.1906.08172},
	shorttitle = {{MediaPipe}},
	abstract = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The {MediaPipe} framework addresses all of these challenges. A developer can use {MediaPipe} to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use {MediaPipe} as an environment for iteratively improving their application with results reproducible across different devices and platforms. {MediaPipe} will be open-sourced at https://github.com/google/mediapipe.},
	publisher = {{arXiv}},
	author = {Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and {McClanahan}, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and Chang, Wan-Teh and Hua, Wei and Georg, Manfred and Grundmann, Matthias},
	urldate = {2025-06-20},
	date = {2019},
	langid = {english},
	note = {Version Number: 1},
	keywords = {{FOS}: Computer and information sciences, Distributed, Parallel, and Cluster Computing (cs.{DC})},
}

@inproceedings{gil-martin_hand_2025,
	title = {Hand gesture recognition using {MediaPipe} landmarks and deep learning networks},
	pages = {24--30},
	booktitle = {In Proceedings of the 17th International Conference on Agents and Artificial Intelligence-volume 3: {ICAART}},
	author = {Gil-Martín, Manuel and Marini, Marco and Martín-Fernández, Iván and Esteban-Romero, Sergio and Cinque, Luigi and {Others}},
	date = {2025},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{arya_enhancing_2024,
	location = {Delhi, India},
	title = {Enhancing human pose estimation: a data-driven approach with {MediaPipe} {BlazePose} and feature engineering analysing},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-6501-6},
	url = {https://ieeexplore.ieee.org/document/10696215/},
	doi = {10.1109/IC2SDT62152.2024.10696215},
	shorttitle = {Enhancing human pose estimation},
	eventtitle = {2024 First International Conference on Pioneering Developments in Computer Science \&amp; Digital Technologies ({IC}2SDT)},
	pages = {1--6},
	booktitle = {2024 First International Conference on Pioneering Developments in Computer Science \&amp; Digital Technologies ({IC}2SDT)},
	publisher = {{IEEE}},
	author = {Arya, Vishakha and Maji, Srabanti},
	urldate = {2025-06-20},
	date = {2024-08-02},
	langid = {english},
	note = {{TLDR}: Identifying human activities by detecting specific vital points and assessing the accuracy of their movements, which have been empirically shown to produce reliable outcomes to help enhance overall effectiveness are explored.},
}

@article{fang_alphapose_2022,
	title = {Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time},
	volume = {45},
	doi = {10.1109/TPAMI.2022.3222784},
	pages = {7157--7173},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Fang, Hao-Shu and Li, Jiefeng and Tang, Hongyang and Xu, Chao and Zhu, Haoyi and Xiu, Yuliang and Li, Yong-Lu and Lu, Cewu},
	date = {2022},
	note = {Publisher: {IEEE}
{TLDR}: This article presents {AlphaPose}, a system that can perform accurate whole-body pose estimation and tracking jointly while running in realtime, and proposes several new techniques: Symmetric Integral Keypoint Regression ({SIKR}) for fast and fine localization, Parametric Pose Non-Maximum-Suppression (P-{NMS}) for eliminating redundant human detections and Pose Aware Identity Embedding for jointly posing and tracking.},
}

@article{huang_automatic_2025,
	title = {Automatic detection of teacher behavior in classroom videos using {AlphaPose} and faster R-{CNN} algorithms},
	volume = {11},
	pages = {e2933},
	journaltitle = {{PeerJ} Computer Science},
	shortjournal = {{PeerJ} Comput. Sci.},
	author = {Huang, Jing and Hashim, Harwati and Norman, Helmi and Zaini, Mohammad Hafiz and Zhang, Xiaojun},
	date = {2025},
	langid = {english},
	note = {Publisher: {PeerJ} Inc.},
}

@article{liu_hybrid_2025,
	title = {A hybrid human fall detection method based on modified {YOLOv}8s and {AlphaPose}},
	volume = {15},
	pages = {2636},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci. Rep.},
	author = {Liu, Lei and Sun, Yeguo and Li, Yinyin and Liu, Yihong},
	date = {2025},
	langid = {english},
	note = {Publisher: Nature Publishing Group {UK} London},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{lin_microsoft_2014,
	title = {Microsoft coco: common objects in context},
	pages = {740--755},
	booktitle = {Computer Vision–eccv 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
	publisher = {Springer},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C Lawrence},
	date = {2014},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{johnson_clustered_2010,
	title = {Clustered pose and nonlinear appearance models for human pose estimation},
	doi = {10.5244/C.24.12},
	booktitle = {Proceedings of the British Machine Vision Conference},
	author = {Johnson, Sam and Everingham, Mark},
	date = {2010},
	langid = {english},
	note = {{TLDR}: A new annotated database of challenging consumer images is introduced, an order of magnitude larger than currently available datasets, and over 50\% relative improvement in pose estimation accuracy over a state-of-the-art method is demonstrated.},
	keywords = {Leeds Sports Pose ({LSP})},
}

@online{nelson_you_2020,
	title = {You Might Be Resizing Your Images Incorrectly},
	url = {https://blog.roboflow.com/you-might-be-resizing-your-images-incorrectly/},
	titleaddon = {You Might Be Resizing Your Images Incorrectly},
	author = {Nelson, Joseph},
	date = {2020-01-31},
	langid = {english},
}

@inproceedings{sinha_thin_2019,
	location = {New York City, {NY}, {USA}},
	title = {Thin {MobileNet}: an enhanced {MobileNet} architecture},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-7281-3885-5},
	url = {https://ieeexplore.ieee.org/document/8993089/},
	doi = {10.1109/UEMCON47517.2019.8993089},
	shorttitle = {Thin {MobileNet}},
	eventtitle = {2019 {IEEE} 10th Annual Ubiquitous Computing, Electronics \& Mobile Communication Conference ({UEMCON})},
	pages = {280--285},
	booktitle = {2019 {IEEE} 10th Annual Ubiquitous Computing, Electronics \& Mobile Communication Conference ({UEMCON})},
	publisher = {{IEEE}},
	author = {Sinha, Debjyoti and El-Sharkawy, Mohamed},
	urldate = {2025-06-22},
	date = {2019-10},
	langid = {english},
	note = {{TLDR}: Three hybrid {MobileNet} architectures are proposed which has improved accuracy along-with reduced size, lesser number of layers, lower average computation time and very less overfitting as compared to the baseline {MobileNet} v1.},
	file = {已提交版本:D\:\\Download\\Zotero\\storage\\YYHHASL5\\Sinha和El-Sharkawy - 2019 - Thin MobileNet an enhanced MobileNet architecture.pdf:application/pdf},
}

@article{wang_novel_2020,
	title = {A novel image classification approach via dense-{MobileNet} models},
	volume = {2020},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1574-017X, 1875-905X},
	url = {https://www.hindawi.com/journals/misy/2020/7602384/},
	doi = {10.1155/2020/7602384},
	abstract = {As a lightweight deep neural network, {MobileNet} has fewer parameters and higher classification accuracy. In order to further reduce the number of network parameters and improve the classification accuracy, dense blocks that are proposed in {DenseNets} are introduced into {MobileNet}. In Dense-{MobileNet} models, convolution layers with the same size of input feature maps in {MobileNet} models are taken as dense blocks, and dense connections are carried out within the dense blocks. The new network structure can make full use of the output feature maps generated by the previous convolution layers in dense blocks, so as to generate a large number of feature maps with fewer convolution cores and repeatedly use the features. By setting a small growth rate, the network further reduces the parameters and the computation cost. Two Dense-{MobileNet} models, Dense1-{MobileNet} and Dense2-{MobileNet}, are designed. Experiments show that Dense2-{MobileNet} can achieve higher recognition accuracy than {MobileNet}, while only with fewer parameters and computation cost.},
	pages = {1--8},
	journaltitle = {Mobile Information Systems},
	shortjournal = {Mobile Inf. Syst.},
	author = {Wang, Wei and Li, Yutao and Zou, Ting and Wang, Xin and You, Jieyu and Luo, Yanhong},
	urldate = {2025-06-22},
	date = {2020-01-06},
	langid = {english},
	note = {{TLDR}: Dense blocks that are proposed in {DenseNets} are introduced into {MobileNet} in order to further reduce the number of network parameters and improve the classification accuracy, and two Dense-{MobileNet} models are designed.},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\UCILKZFS\\Wang 等 - 2020 - A novel image classification approach via dense-MobileNet models.pdf:application/pdf},
}

@online{ultralytics_pose_2024,
	title = {Pose Estimation},
	url = {https://docs.ultralytics.com/tasks/pose},
	abstract = {Discover how to use {YOLO}11 for pose estimation tasks. Learn about model training, validation, prediction, and exporting in various formats.},
	author = {Ultralytics},
	urldate = {2025-06-27},
	date = {2024},
	langid = {english},
	file = {Snapshot:D\:\\Download\\Zotero\\storage\\M92DT2RA\\pose.html:text/html},
}

@online{gokamisama_object_2024,
	title = {Object Detection Algorithms of {YOLO} Series},
	url = {https://www.cnblogs.com/marui199710/articles/18448535},
	titleaddon = {Object Detection Algorithms of {YOLO} Series},
	author = {{gokamisama}},
	urldate = {2025-06-27},
	date = {2024},
}

@misc{max_planck_institute_for_informatics_mpii_2020,
	title = {{MPII} Human Pose Data},
	url = {https://www.kaggle.com/datasets/nicolehoelzl/mpii-human-pose-data},
	author = {Max Planck Institute for Informatics},
	date = {2020},
	note = {remark: {CSV} data},
}

@inproceedings{andriluka_2d_2014,
	title = {2D human pose estimation: new benchmark and state of the art analysis},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
	date = {2014-06},
	langid = {english},
	note = {remark: {MPII} images},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} large scale visual recognition challenge},
	volume = {115},
	doi = {10.1007/s11263-015-0816-y},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision ({IJCV})},
	shortjournal = {Int. J. Comput. Vis. ({IJCV})},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	date = {2015},
	langid = {english},
	note = {remark: {ImageNet}
{TLDR}: The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared.},
}

@misc{pratapdevs11_gym_2024,
	title = {Gym Data coco format For {YOLO}-pose},
	url = {https://www.kaggle.com/datasets/pratapdevs11/gym-data-coco-format-for-yolo-pose},
	author = {pratapdevs11},
	urldate = {2025-06-28},
	date = {2024},
}

@misc{everingham_pascal_nodate,
	title = {The {PASCAL} visual object classes challenge 2012 ({VOC}2012) results},
	url = {http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html},
	author = {Everingham, M. and Van Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
	langid = {english},
	note = {remark: {PASCAL} {VOC} 2012},
}

@online{noauthor_yolo-pose_nodate,
	title = {{YOLO}-Pose: Human Body Keypoint Estimation},
}

@misc{maji_yolo-pose_2022,
	title = {{YOLO}-pose: enhancing {YOLO} for multi person pose estimation using object keypoint similarity loss},
	url = {http://arxiv.org/abs/2204.06806},
	doi = {10.48550/arXiv.2204.06806},
	shorttitle = {{YOLO}-pose},
	abstract = {We introduce {YOLO}-pose, a novel heatmap-free approach for joint detection, and 2D multi-person pose estimation in an image based on the popular {YOLO} object detection framework. Existing heatmap based two-stage approaches are sub-optimal as they are not end-to-end trainable and training relies on a surrogate L1 loss that is not equivalent to maximizing the evaluation metric, i.e. Object Keypoint Similarity ({OKS}). Our framework allows us to train the model end-to-end and optimize the {OKS} metric itself. The proposed model learns to jointly detect bounding boxes for multiple persons and their corresponding 2D poses in a single forward pass and thus bringing in the best of both top-down and bottom-up approaches. Proposed approach doesn't require the postprocessing of bottom-up approaches to group detected keypoints into a skeleton as each bounding box has an associated pose, resulting in an inherent grouping of the keypoints. Unlike top-down approaches, multiple forward passes are done away with since all persons are localized along with their pose in a single inference. {YOLO}-pose achieves new state-of-the-art results on {COCO} validation (90.2\% {AP}50) and test-dev set (90.3\% {AP}50), surpassing all existing bottom-up approaches in a single forward pass without flip test, multi-scale testing, or any other test time augmentation. All experiments and results reported in this paper are without any test time augmentation, unlike traditional approaches that use flip-test and multi-scale testing to boost performance. Our training codes will be made publicly available at https://github.com/{TexasInstruments}/edgeai-yolov5 and https://github.com/{TexasInstruments}/edgeai-yolox},
	number = {{arXiv}:2204.06806},
	publisher = {{arXiv}},
	author = {Maji, Debapriya and Nagori, Soyeb and Mathew, Manu and Poddar, Deepak},
	urldate = {2025-06-28},
	date = {2022-04-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2204.06806 [cs]},
	note = {{TLDR}: {YOLO}-pose is introduced, a novel heatmap-free approach for joint detection, and 2D multi-person pose estimation in an image based on the popular {YOLO} object detection framework, surpassing all existing bottom-up approaches in a single forward pass without flip test, multi-scale testing, or any other test time augmentation.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\MHNQE6ZP\\Maji 等 - 2022 - YOLO-Pose Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss.pdf:application/pdf;Snapshot:D\:\\Download\\Zotero\\storage\\K8K34NIG\\2204.html:text/html},
}

@online{tseng_yolov8_2023,
	title = {{YOLOV}8 Pose Estimation},
	url = {https://chtseng.wordpress.com/2023/07/14/yolov8-pose-estimation/},
	author = {Tseng, {CH}},
	date = {2023-07-14},
}

@misc{lin_focal_2017,
	title = {Focal loss for dense object detection},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1708.02002},
	doi = {10.48550/ARXIV.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-{CNN}, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call {RetinaNet}. Our results show that when trained with the focal loss, {RetinaNet} is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	publisher = {{arXiv}},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	urldate = {2025-06-29},
	date = {2017},
	langid = {english},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\WJLXE5BX\\Lin 等 - 2017 - Focal loss for dense object detection.pdf:application/pdf},
}

@inproceedings{redmon_you_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {You only look once: unified, real-time object detection},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/cvpr.2016.91},
	shorttitle = {You only look once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {779--788},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2025-07-10},
	date = {2016-06},
	langid = {english},
	note = {{TLDR}: Compared to state-of-the-art detection systems, {YOLO} makes more localization errors but is less likely to predict false positives on background, and outperforms other detection methods, including {DPM} and R-{CNN}, when generalizing from natural images to other domains like artwork.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\BVHEBGB9\\Redmon 等 - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:application/pdf},
}

@inproceedings{redmon_yolo9000_2017,
	location = {Honolulu, {HI}},
	title = {{YOLO}9000: better, faster, stronger},
	url = {http://ieeexplore.ieee.org/document/8100173/},
	doi = {10.1109/cvpr.2017.690},
	shorttitle = {Yolo9000},
	abstract = {We introduce {YOLO}9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the {YOLO} detection method, both novel and drawn from prior work. The improved model, {YOLOv}2, is state-of-the-art on standard detection tasks like {PASCAL} {VOC} and {COCO}. Using a novel, multi-scale training method the same {YOLOv}2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 {FPS}, {YOLOv}2 gets 76.8 {mAP} on {VOC} 2007. At 40 {FPS}, {YOLOv}2 gets 78.6 {mAP}, outperforming state-of-the-art methods like Faster {RCNN} with {ResNet} and {SSD} while still running signiﬁcantly faster. Finally we propose a method to jointly train on object detection and classiﬁcation. Using this method we train {YOLO}9000 simultaneously on the {COCO} detection dataset and the {ImageNet} classiﬁcation dataset. Our joint training allows {YOLO}9000 to predict detections for object classes that don’t have labelled detection data. We validate our approach on the {ImageNet} detection task. {YOLO}9000 gets 19.7 {mAP} on the {ImageNet} detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in {COCO}, {YOLO}9000 gets 16.0 {mAP}. {YOLO}9000 predicts detections for more than 9000 different object categories, all in real-time.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {6517--6525},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2025-07-10},
	date = {2017-07},
	langid = {english},
	note = {{TLDR}: {YOLO}9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories, is introduced and a method to jointly train on object detection and classification is proposed, both novel and drawn from prior work.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\W2EY7C33\\Redmon和Farhadi - 2017 - YOLO9000 Better, Faster, Stronger.pdf:application/pdf},
}

@misc{redmon_yolov3_2018,
	title = {{YOLOv}3: an incremental improvement},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1804.02767},
	doi = {10.48550/ARXIV.1804.02767},
	shorttitle = {{YOLOv}3},
	abstract = {We present some updates to {YOLO}! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 {YOLOv}3 runs in 22 ms at 28.2 {mAP}, as accurate as {SSD} but three times faster. When we look at the old .5 {IOU} {mAP} detection metric {YOLOv}3 is quite good. It achieves 57.9 {mAP}@50 in 51 ms on a Titan X, compared to 57.5 {mAP}@50 in 198 ms by {RetinaNet}, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	publisher = {{arXiv}},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2025-07-10},
	date = {2018},
	langid = {english},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\56227T45\\Redmon和Farhadi - 2018 - YOLOv3 an incremental improvement.pdf:application/pdf},
}

@software{jocher_ultralytics_2023,
	title = {Ultralytics {YOLO}},
	url = {https://github.com/ultralytics/ultralytics},
	version = {8.0.0},
	author = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
	date = {2023-01},
}

@article{lei_yolov13_2025,
	title = {{YOLOv}13: real-time object detection with hypergraph-enhanced adaptive visual perception},
	doi = {https://doi.org/10.48550/arXiv.2506.17733},
	journaltitle = {Arxiv Preprint Arxiv:2506.17733},
	shortjournal = {Arxiv Prepr. Arxiv:2506,17733},
	author = {Lei, Mengqi and Li, Siqi and Wu, Yihong and Al, Et},
	date = {2025},
	langid = {english},
	note = {{titleTranslation}: {YOLOv}13：使用 Hypergraph 增强的自适应视觉感知进行实时对象检测},
	keywords = {⛔ No {DOI} found},
	file = {Preprint PDF:D\:\\Download\\Zotero\\storage\\TWMJCMXM\\Lei 等 - 2025 - YOLOv13 real-time object detection with hypergraph-enhanced adaptive visual perception.pdf:application/pdf},
}

@article{abelaira_current_2025,
	title = {Current perspectives on responsible digitalization: a bibliometric review of the concept},
	volume = {17},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/17/5/1915},
	doi = {10.3390/su17051915},
	shorttitle = {Current perspectives on responsible digitalization},
	abstract = {Digitalization, digitalization, or digital transformation is a phenomenon without which it would be difficult to understand the reality of our time. Although it is often associated with the incorporation of technology into business, the economy, or our own lives, it goes further by involving a transformation process that can significantly improve sustainable corporate development. The work, from the perspective of bibliometric analysis, maps the state of the art in the area of study of digitization, digitization, digital transformation, and sustainability from 72 articles obtained from the Web of Science database, dating from the beginning of time to the year 2024, without limiting our search to a particular type of document or discriminating by year of publication. The software used to carry out this bibliometric analysis was {SciMAT}. The results allow us to establish digitalization as an area of research that is in full development and a link between different areas of research, with the areas of “Environmental Sciences” and "Green Sustainable Science Technology" being the most often addressed by academics. In addition, this study identifies digitalization and the blockchain as driving themes, leaving other topics such as challenges, artificial intelligence, information technology, or digital transformation with less importance. This allows researchers to expand existing knowledge in the field, position themselves in areas of high relevance and potential impact, and be a starting point for future research.},
	pages = {1915},
	number = {5},
	journaltitle = {Sustainability},
	shortjournal = {Sustainability},
	author = {Abelaira, Triana Arias and Caro, Carlos Diaz and Sanguino, Ángel Sabino Mirón and Ariza, Lázaro Rodriguez},
	urldate = {2025-07-16},
	date = {2025-02-24},
	langid = {english},
	note = {Publisher: {MDPI} {AG}
{TLDR}: The work maps the state of the art in the area of study of digitization, digitization, digital transformation, and sustainability from 72 articles obtained from the Web of Science database, dating from the beginning of time to the year 2024.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\CR9NV35N\\Abelaira 等 - 2025 - Current Perspectives on Responsible Digitalization A Bibliometric Review of the Concept.pdf:application/pdf},
}

@article{mas-tur_advances_2020,
	title = {Advances in management research: a bibliometric overview of the review of managerial science},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0},
	issn = {1863-6683, 1863-6691},
	url = {https://link.springer.com/10.1007/s11846-020-00406-z},
	doi = {10.1007/s11846-020-00406-z},
	shorttitle = {Advances in management research},
	abstract = {The Review of Managerial Science ({RMS}) is a leading international journal that publishes major advances related to business administration and management. The journal was launched in April 2007 and publishes eight issues per year (from 2021 onwards). The scope of {RMS} encompasses, but is not limited to, the functional areas of operations (such as production, operations management, and marketing), management (such as human resources management, strategic management, and organizational theory), information systems and their interrelations with capital markets (such as accounting, auditing, finance, and taxation), as well as questions of business strategy, entrepreneurship, innovation, and corporate governance. This study offers a bibliometric overview of the publication and citation structure of {RMS} from its inception in 2007 until 2020 in terms of topics, authors, institutions, and countries, thereby offering a comprehensive overview of the history of the journal so far. All the data for the study are from the Web of Science Core Collection database. To complement this analysis, {VOSviewer} software provides graphical analysis. The analysis is based on several bibliometric techniques such as co-citation analysis and bibliographic coupling.},
	pages = {933--958},
	number = {5},
	journaltitle = {Review of Managerial Science},
	shortjournal = {Rev. Manag. Sci.},
	author = {Mas-Tur, Alicia and Kraus, Sascha and Brandtner, Mario and Ewert, Ralf and Kürsten, Wolfgang},
	urldate = {2025-07-16},
	date = {2020-10},
	langid = {english},
	note = {Publisher: Springer Science and Business Media {LLC}},
	file = {PDF:D\:\\Download\\Zotero\\storage\\EDACK7SW\\Mas-Tur 等 - 2020 - Advances in management research a bibliometric overview of the Review of Managerial Science.pdf:application/pdf},
}

@software{static__nodate,
	title = {阅读历史记录},
	url = {https://github.com/volatile-static/Chartero},
	shorttitle = {chartero},
	abstract = {Chartero记录的阅读历史数据。
请勿修改本条目！（可以移动）},
	author = {Static, Volatile},
}

@article{das_deep_2025,
	title = {Deep learning-based classification, detection, and segmentation of tomato leaf diseases: a state-of-the-art review},
	volume = {15},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {2589-7217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S258972172500025X},
	doi = {10.1016/j.aiia.2025.02.006},
	shorttitle = {Deep learning-based classification, detection, and segmentation of tomato leaf diseases},
	abstract = {The early identiﬁcation and treatment of tomato leaf diseases are crucial for optimizing plant productivity, efﬁciency and quality. Misdiagnosis by the farmers poses the risk of inadequate treatments, harming both tomato plants and agroecosystems. Precision of disease diagnosis is essential, necessitating a swift and accurate response to misdiagnosis for early identiﬁcation. Tropical regions are ideal for tomato plants, but there are inherent concerns, such as weather-related problems. Plant diseases largely cause ﬁnancial losses in crop production. The slow detection periods of conventional approaches are insufﬁcient for the timely detection of tomato diseases. Deep learning has emerged as a promising avenue for early disease identiﬁcation. This study comprehensively analyzed techniques for classifying and detecting tomato leaf diseases and evaluating their strengths and weaknesses. The study delves into various diagnostic procedures, including image pre-processing, localization and segmentation. In conclusion, applying deep learning algorithms holds great promise for enhancing the accuracy and efﬁciency of tomato leaf disease diagnosis by offering faster and more effective results.},
	pages = {192--220},
	number = {2},
	journaltitle = {Artificial Intelligence in Agriculture},
	shortjournal = {Artif. Intell. Agric.},
	author = {Das, Aritra and Pathan, Fahad and Jim, Jamin Rahman and Kabir, Md Mohsin and Mridha, M.F.},
	urldate = {2025-07-16},
	date = {2025-06},
	langid = {english},
	note = {Publisher: Elsevier {BV}},
	file = {PDF:D\:\\Download\\Zotero\\storage\\FCQJMFGA\\Das 等 - 2025 - Deep learning-based classification, detection, and segmentation of tomato leaf diseases a state-of-.pdf:application/pdf},
}

@article{george_past_2025,
	title = {Past, present and future of deep plant leaf disease recognition: a survey},
	volume = {234},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0168-1699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169925002340},
	doi = {10.1016/j.compag.2025.110128},
	shorttitle = {Past, present and future of deep plant leaf disease recognition},
	pages = {110128},
	journaltitle = {Computers and Electronics in Agriculture},
	shortjournal = {Comput. Electron. Agric.},
	author = {George, Romiyal and Thuseethan, Selvarajah and Ragel, Roshan G. and Mahendrakumaran, Kayathiri and Nimishan, Sivaraj and Wimalasooriya, Chathrie and Alazab, Mamoun},
	urldate = {2025-07-16},
	date = {2025-07},
	langid = {english},
	note = {Publisher: Elsevier {BV}},
	file = {Past present and future of deep plant leaf disease recognition A survey:D\:\\Download\\Zotero\\storage\\98RQUY76\\Past present and future of deep plant leaf disease recognition A survey.pdf:application/pdf},
}

@misc{jegham_yolo_2025,
	title = {{YOLO} evolution: a comprehensive benchmark and architectural review of {YOLOv}12, {YOLO}11, and their previous versions},
	url = {http://arxiv.org/abs/2411.00201},
	doi = {10.48550/arXiv.2411.00201},
	shorttitle = {{YOLO} evolution},
	abstract = {This study presents a comprehensive benchmark analysis of various {YOLO} (You Only Look Once) algorithms. It represents the first comprehensive experimental evaluation of {YOLOv}3 to the latest version, {YOLOv}12, on various object detection challenges. The challenges considered include varying object sizes, diverse aspect ratios, and small-sized objects of a single class, ensuring a comprehensive assessment across datasets with distinct challenges. To ensure a robust evaluation, we employ a comprehensive set of metrics, including Precision, Recall, Mean Average Precision ({mAP}), Processing Time, {GFLOPs} count, and Model Size. Our analysis highlights the distinctive strengths and limitations of each {YOLO} version. For example: {YOLOv}9 demonstrates substantial accuracy but struggles with detecting small objects and efficiency whereas {YOLOv}10 exhibits relatively lower accuracy due to architectural choices that affect its performance in overlapping object detection but excels in speed and efficiency. Additionally, the {YOLO}11 family consistently shows superior performance maintaining a remarkable balance of accuracy and efficiency. However, {YOLOv}12 delivered underwhelming results, with its complex architecture introducing computational overhead without significant performance gains. These results provide critical insights for both industry and academia, facilitating the selection of the most suitable {YOLO} algorithm for diverse applications and guiding future enhancements.},
	number = {{arXiv}:2411.00201},
	publisher = {{arXiv}},
	author = {Jegham, Nidhal and Koh, Chan Young and Abdelatti, Marwan and Hendawi, Abdeltawab},
	urldate = {2025-07-16},
	date = {2025-03-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2411.00201 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:D\:\\Download\\Zotero\\storage\\6NTG6NJL\\Jegham 等 - 2025 - YOLO Evolution A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Pre.pdf:application/pdf},
}

@online{noauthor_grammarly_nodate,
	title = {Grammarly},
	url = {https://app.grammarly.com/apps},
	urldate = {2025-08-08},
	langid = {english},
	file = {Grammarly:D\:\\Download\\Zotero\\storage\\MYJXDZZC\\apps.html:text/html},
}

@software{noauthor_addon_nodate,
	title = {Addon item},
}

@article{yang_application_2024,
	title = {Application of online teaching-based classroom behavior capture and analysis system in student management},
	volume = {33},
	rights = {http://creativecommons.org/licenses/by/4.0},
	issn = {2191-026X},
	url = {https://www.degruyter.com/document/doi/10.1515/jisys-2023-0236/html},
	doi = {10.1515/jisys-2023-0236},
	abstract = {Analyzing online learning behavior helps to understand students’ progress, diﬃculties, and needs during the learning process, making it easier for teachers to provide timely feedback and personalized guidance. However, the classroom behavior ({CB}) of online teaching is complex and variable, and relying on traditional classroom supervision methods, teachers ﬁnd it diﬃcult to comprehensively pay attention to the learning behavior of each student. In this regard, a dual stream network was designed to capture and analyze {CB} by integrating {AlphaPose} human keypoint detection method and image data method. The experimental results show that when the learning rate of the model parameters is set to 0.001, the accuracy of the model is as high as 92.3\%. When the batch size is 8, the accuracy of the model is as high as 90.8\%. The accuracy of the fusion model in capturing upright sitting behavior reached 97.3\%, but the accuracy in capturing hand raising behavior decreased to only 74.8\%. The fusion model performs well in terms of accuracy and recall, with recall rates of 88.3, 86.2, and 85.1\% for capturing standing up, raising hands, and sitting upright behaviors, respectively. And the maximum F1 value is 0.931. The dual stream network eﬀectively integrates the advantages of two types of data, improves the performance of behavior capture, and improves the robustness of the algorithm. The successful application of the model is beneﬁcial for teachers’ classroom observation and research activities, providing a favorable path for their professional development, and thereby improving the overall teaching quality of teachers.},
	pages = {20230236},
	number = {1},
	journaltitle = {Journal of Intelligent Systems},
	shortjournal = {J. Intell. Syst.},
	author = {Yang, Liu},
	urldate = {2025-08-11},
	date = {2024-06-06},
	langid = {english},
	note = {{TLDR}: A dual stream network designed to capture and analyze {CB} by integrating {AlphaPose} human keypoint detection method and image data method effectively integrates the advantages of two types of data, improves the performance of behavior capture, and improves the robustness of the algorithm is successful.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\G62679WS\\Yang - 2024 - Application of online teaching-based classroom behavior capture and analysis system in student manag.pdf:application/pdf},
}

@article{li_systematic_2025,
	title = {Systematic review of motion capture in virtual reality: enhancing the precision of sports training},
	volume = {17},
	issn = {1876-1364, 1876-1372},
	url = {https://journals.sagepub.com/doi/10.3233/AIS-230198},
	doi = {10.3233/AIS-230198},
	shorttitle = {Systematic review of motion capture in virtual reality},
	abstract = {In the modern era of sports training, the synergy between motion capture and Virtual Reality ({VR}) offers an innovative approach to enhancing training precision. This systematic review delves into the application of motion capture within {VR} for sports training, highlighting its transformative potential. Through a comprehensive literature search, we examined the myriad applications, from physical conditioning enhancements to accelerated rehabilitation processes. Our findings underscore the capability of real-time feedback, immersive training environments, and tailored regimes that this fusion provides. However, despite its promise, challenges such as hardware constraints, data processing complexities, and interaction interface limitations persist. Future trajectories indicate an increasing influence of {AI} and deep learning, promising more sophisticated hardware and a broader spectrum of applications, including niche sports disciplines. The review concludes with an emphasis on the wider societal implications, suggesting a shift towards a holistic athlete well-being approach.},
	pages = {5--27},
	number = {1},
	journaltitle = {Journal of Ambient Intelligence and Smart Environments},
	shortjournal = {J. Ambient Intell. Smart Environ.},
	author = {Li, Xiaohui and Fan, Dongfang and Feng, Junjie and Lei, Yu and Cheng, Chao and Li, Xiangnan},
	urldate = {2025-08-11},
	date = {2025-02},
	langid = {english},
	note = {{TLDR}: This systematic review delves into the application of motion capture within {VR} for sports training, highlighting its transformative potential and placing an emphasis on the wider societal implications, suggesting a shift towards a holistic athlete well-being approach.},
}

@article{schreiter_thor-magni_2025,
	title = {{THÖR}-{MAGNI}: a large-scale indoor motion capture recording of human movement and robot interaction},
	volume = {44},
	issn = {0278-3649, 1741-3176},
	url = {https://journals.sagepub.com/doi/10.1177/02783649241274794},
	doi = {10.1177/02783649241274794},
	shorttitle = {Thör-magni},
	abstract = {We present a new large dataset of indoor human and robot navigation and interaction, called {THO}¨ R-{MAGNI}, that is designed to facilitate research on social human navigation: for example, modeling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. {TH} ¨{OR}-{MAGNI} was created to ﬁll a gap in available datasets for human motion analysis and {HRI}. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, {THO}¨ R-{MAGNI} includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human–human and human–robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze-tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. {THO}¨ R-{MAGNI} is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human–robot interactions.},
	pages = {568--591},
	number = {4},
	journaltitle = {International Journal of Robotics Research},
	shortjournal = {Int. J. Rob. Res.},
	author = {Schreiter, Tim and Rodrigues De Almeida, Tiago and Zhu, Yufei and Gutierrez Maestro, Eduardo and Morillo-Mendez, Lucas and Rudenko, Andrey and Palmieri, Luigi and Kucner, Tomasz P and Magnusson, Martin and Lilienthal, Achim J},
	urldate = {2025-08-11},
	date = {2025-04},
	langid = {english},
	note = {{TLDR}: {THÖR}-{MAGNI} is, to the best of the knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human–robot interactions.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\FKSHEYHP\\Schreiter 等 - 2025 - THÖR-MAGNI A large-scale indoor motion capture recording of human movement and robot interaction.pdf:application/pdf},
}

@article{falisse_marker_2025,
	title = {Marker data enhancement for markerless motion capture},
	volume = {72},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {0018-9294, 1558-2531},
	url = {https://ieeexplore.ieee.org/document/10844513/},
	doi = {10.1109/TBME.2025.3530848},
	abstract = {Objective: Human pose estimation models can measure movement from videos at a large scale and low cost; however, open-source pose estimation models typically detect only sparse keypoints, which leads to inaccurate joint kinematics. {OpenCap}, a freely available service for researchers to measure movement from videos, mitigates this issue using a deep learning model—the marker enhancer—that transforms sparse video keypoints into dense anatomical markers. However, {OpenCap} performs poorly on movements not included in the training data. Here, we create a much larger and more diverse training dataset and develop a more accurate and generalizable marker enhancer. Methods: We compiled marker-based motion capture data from 1176 subjects and synthesized 1433 hours of video keypoints and anatomical markers to train the marker enhancer. We evaluated its accuracy in computing kinematics using both benchmark movement videos and synthetic data representing unseen, diverse movements. Results: The marker enhancer improved kinematic accuracy on benchmark movements (mean error: 4.1◦, max: 8.7◦) compared to using video keypoints (mean: 9.6◦, max: 43.1◦) and {OpenCap}’s original enhancer (mean: 5.3◦, max: 11.5◦). It also better generalized to unseen, diverse movements (mean: 4.1◦, max: 6.7◦) than {OpenCap}’s original enhancer (mean: 40.4◦, max: 252.0◦). Conclusion: Our marker enhancer demonstrates both improved accuracy and generalizability across diverse movements. Signiﬁcance: We integrated the marker enhancer into {OpenCap}, thereby offering its thousands of users more accurate measurements across a broader range of movements.},
	pages = {2013--2022},
	number = {6},
	journaltitle = {{IEEE} Transactions on Bio-Medical Engineering},
	shortjournal = {{IEEE} Trans. Bio-Med. Eng.},
	author = {Falisse, Antoine and Uhlrich, Scott D. and Chaudhari, Akshay S. and Hicks, Jennifer L. and Delp, Scott L.},
	urldate = {2025-08-11},
	date = {2025-06},
	langid = {english},
	file = {PDF:D\:\\Download\\Zotero\\storage\\4AAVY3YE\\Falisse 等 - 2025 - Marker Data Enhancement for Markerless Motion Capture.pdf:application/pdf},
}

@article{muramatsu_wildpose_2025,
	title = {{WildPose}: a long-range 3D wildlife motion capture system},
	volume = {228},
	rights = {http://www.biologists.com/user-licence-1-1/},
	issn = {0022-0949, 1477-9145},
	url = {https://journals.biologists.com/jeb/article/228/5/JEB249987/367271/WildPose-a-long-range-3D-wildlife-motion-capture},
	doi = {10.1242/jeb.249987},
	shorttitle = {{WildPose}},
	abstract = {{ABSTRACT}
            Understanding and monitoring wildlife behavior is crucial in ecology and biomechanics, yet challenging because of the limitations of current methods. To address this issue, we introduce {WildPose}, a novel long-range motion capture system specifically tailored for free-ranging wildlife observation. This system combines an electronically controllable zoom-lens camera with a {LiDAR} to capture both 2D videos and 3D point cloud data, thereby allowing researchers to observe high-fidelity animal morphometrics, behavior and interactions in a completely remote manner. Field trials conducted in Kgalagadi Transfrontier Park (South Africa) have successfully demonstrated {WildPose}'s ability to quantify morphological features of different species, accurately track the 3D movements of a springbok herd over time, and observe the respiratory patterns of a distant lion. By facilitating non-intrusive, long-range 3D data collection, {WildPose} marks a significant complementary technique in ecological and biomechanical studies, offering new possibilities for conservation efforts and animal welfare, and enriching the prospects for interdisciplinary research.},
	pages = {JEB249987},
	number = {5},
	journaltitle = {Journal of Experimental Biology},
	shortjournal = {J. Exp. Biol.},
	author = {Muramatsu, Naoya and Shin, Sangyun and Deng, Qianyi and Markham, Andrew and Patel, Amir},
	urldate = {2025-08-11},
	date = {2025-03-01},
	langid = {english},
	file = {Full Text PDF:D\:\\Download\\Zotero\\storage\\C9YTHQXC\\Muramatsu 等 - 2025 - WildPose a long-range 3D wildlife motion capture system.pdf:application/pdf},
}

@article{wang_deep_2020,
	title = {Deep learning based target detection algorithm for motion capture applications},
	volume = {1682},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1682/1/012032},
	doi = {10.1088/1742-6596/1682/1/012032},
	abstract = {Abstract
            Motion capture technology is the use of external devices to perform data recording and posture reproduction of the displacement of human structures. Deep learning algorithms are playing an increasingly important role in motion capture technology as the technology involves data that can be directly understood and processed by computers in terms of dimensional measurements, positioning of objects in physical space, and orientation determination. This paper presents an application of a convolutional neural network system, {YOLO}-V4, in the field of motion capture. {YOLO}-V4 system weight files are small and do not require high hardware requirements. It can also be implemented in {PyTorch} so that it can be deployed on mobile devices, enabling edge devices to run these models as well, relieving the space constraint of immovable signal capture devices and providing the advantages of high accuracy and high detection rate.},
	pages = {12032},
	number = {1},
	journaltitle = {Journal of Physics: Conference Series},
	shortjournal = {J. Phys. Conf. Ser.},
	author = {Wang, Haitao and Tong, Xin and Lu, Fengyun},
	urldate = {2025-08-11},
	date = {2020-11-01},
	langid = {english},
	note = {{TLDR}: {YOLO}-V4 system weight files are small and do not require high hardware requirements, enabling edge devices to run these models as well, relieving the space constraint of immovable signal capture devices and providing the advantages of high accuracy and high detection rate.},
	file = {全文:D\:\\Download\\Zotero\\storage\\YWUZQDYW\\Wang 等 - 2020 - Deep learning based target detection algorithm for motion capture applications.pdf:application/pdf},
}

@inproceedings{chen_yolo_2024,
	location = {Bhubaneswar, India},
	title = {{YOLO} algorithm in analysis and design of athletes’ actions in college physical education},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-7442-1},
	url = {https://ieeexplore.ieee.org/document/10734504/},
	doi = {10.1109/IIST62526.2024.00002},
	eventtitle = {2024 International Conference on Interactive Intelligent Systems and Techniques ({IIST})},
	pages = {764--768},
	booktitle = {2024 International Conference on Interactive Intelligent Systems and Techniques ({IIST})},
	publisher = {{IEEE}},
	author = {Chen, Hailong},
	urldate = {2025-08-11},
	date = {2024-03-04},
	langid = {english},
	note = {{TLDR}: The results indicate that the action analysis based on {YOLO} algorithm can effectively improve the fluency and safety of students’ actions.},
}

@article{alzahrani_yolo-act_2025,
	title = {{YOLO}-act: unified spatiotemporal detection of human actions across multi-frame sequences},
	volume = {25},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/10/3013},
	doi = {10.3390/s25103013},
	shorttitle = {{YOLO}-act},
	abstract = {Automated action recognition has become essential in the surveillance, healthcare, and multimedia retrieval industries owing to the rapid proliferation of video data. This paper introduces {YOLO}-Act, a novel spatiotemporal action detection model that enhances the object detection capabilities of {YOLOv}8 to efficiently manage complex action dynamics within video sequences. {YOLO}-Act achieves precise and efficient action recognition by integrating keyframe extraction, action tracking, and class fusion. The model depicts essential temporal dynamics without the computational overhead of continuous frame processing by leveraging the adaptive selection of three keyframes representing the beginning, middle, and end of the actions. Compared with state-of-the-art approaches such as the Lagrangian Action Recognition Transformer ({LART}), {YOLO}-Act exhibits superior performance with a mean average precision ({mAP}) of 73.28 in experiments conducted on the {AVA} dataset, resulting in a gain of +28.18 {mAP}. Furthermore, {YOLO}-Act achieves this higher accuracy with significantly lower {FLOPs}, demonstrating its efficiency in computational resource utilization. The results highlight the advantages of incorporating precise tracking, effective spatial detection, and temporal consistency to address the challenges associated with video-based action detection.},
	pages = {3013},
	number = {10},
	journaltitle = {Sensors},
	shortjournal = {Sens.},
	author = {Alzahrani, Nada and Bchir, Ouiem and Ismail, Mohamed Maher Ben},
	urldate = {2025-08-11},
	date = {2025-05-10},
	langid = {english},
	note = {{TLDR}: {YOLO}-Act is introduced, a novel spatiotemporal action detection model that enhances the object detection capabilities of {YOLOv}8 to efficiently manage complex action dynamics within video sequences to address the challenges associated with video-based action detection.},
	file = {PDF:D\:\\Download\\Zotero\\storage\\KHBSHEE6\\Alzahrani 等 - 2025 - YOLO-Act Unified Spatiotemporal Detection of Human Actions Across Multi-Frame Sequences.pdf:application/pdf},
}
